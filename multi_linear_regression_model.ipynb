{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>param</th>\n",
       "      <th>gpu</th>\n",
       "      <th>FLOPS</th>\n",
       "      <th>class_num</th>\n",
       "      <th>t_img_num</th>\n",
       "      <th>t_instance_num</th>\n",
       "      <th>v_img_num</th>\n",
       "      <th>v_instance_num</th>\n",
       "      <th>imgsz</th>\n",
       "      <th>...</th>\n",
       "      <th>preprocess_time</th>\n",
       "      <th>train_time</th>\n",
       "      <th>val_time</th>\n",
       "      <th>save_model_time</th>\n",
       "      <th>scheduler_time</th>\n",
       "      <th>epoch_time</th>\n",
       "      <th>elapsed</th>\n",
       "      <th>remaining</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>cpu_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yolov8l</td>\n",
       "      <td>43643718</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yolov8l</td>\n",
       "      <td>43643718</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>114.019438</td>\n",
       "      <td>2.826854</td>\n",
       "      <td>4.204662</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>121.066030</td>\n",
       "      <td>121.066030</td>\n",
       "      <td>11985</td>\n",
       "      <td>1.756200</td>\n",
       "      <td>1.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yolov8l</td>\n",
       "      <td>43643718</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>114.627127</td>\n",
       "      <td>2.597382</td>\n",
       "      <td>6.165627</td>\n",
       "      <td>0.015966</td>\n",
       "      <td>122.241334</td>\n",
       "      <td>244.482668</td>\n",
       "      <td>11999</td>\n",
       "      <td>1.767350</td>\n",
       "      <td>1.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yolov8l</td>\n",
       "      <td>43643718</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>112.422867</td>\n",
       "      <td>2.535480</td>\n",
       "      <td>6.449841</td>\n",
       "      <td>0.018609</td>\n",
       "      <td>121.973749</td>\n",
       "      <td>365.921246</td>\n",
       "      <td>11832</td>\n",
       "      <td>1.747733</td>\n",
       "      <td>1.204667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yolov8l</td>\n",
       "      <td>43643718</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>112.300142</td>\n",
       "      <td>2.535398</td>\n",
       "      <td>3.453643</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>121.058286</td>\n",
       "      <td>484.233146</td>\n",
       "      <td>11571</td>\n",
       "      <td>1.765100</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>yolov8s</td>\n",
       "      <td>11142566</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>7.788323</td>\n",
       "      <td>0.606764</td>\n",
       "      <td>1.326586</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>13.981678</td>\n",
       "      <td>1342.241058</td>\n",
       "      <td>70</td>\n",
       "      <td>3.945610</td>\n",
       "      <td>6.341964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>yolov8s</td>\n",
       "      <td>11142566</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>7.735800</td>\n",
       "      <td>0.598938</td>\n",
       "      <td>1.533527</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>13.939540</td>\n",
       "      <td>1352.135335</td>\n",
       "      <td>46</td>\n",
       "      <td>3.945596</td>\n",
       "      <td>6.340059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>yolov8s</td>\n",
       "      <td>11142566</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>7.776962</td>\n",
       "      <td>0.607911</td>\n",
       "      <td>2.577138</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>13.909393</td>\n",
       "      <td>1363.120502</td>\n",
       "      <td>28</td>\n",
       "      <td>3.945758</td>\n",
       "      <td>6.334840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>yolov8s</td>\n",
       "      <td>11142566</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>7.781471</td>\n",
       "      <td>0.596876</td>\n",
       "      <td>1.531579</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>13.869269</td>\n",
       "      <td>1373.057678</td>\n",
       "      <td>12</td>\n",
       "      <td>3.945859</td>\n",
       "      <td>6.327417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>yolov8s</td>\n",
       "      <td>11142566</td>\n",
       "      <td>Tesla V100-PCIE-32GB</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>238</td>\n",
       "      <td>100</td>\n",
       "      <td>235</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>7.779656</td>\n",
       "      <td>0.599159</td>\n",
       "      <td>1.451818</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>13.829164</td>\n",
       "      <td>1382.916353</td>\n",
       "      <td>0</td>\n",
       "      <td>3.945771</td>\n",
       "      <td>6.326000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1515 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name     param                   gpu  FLOPS  class_num  t_img_num  \\\n",
       "0      yolov8l  43643718  Tesla V100-PCIE-32GB     14         18        100   \n",
       "1      yolov8l  43643718  Tesla V100-PCIE-32GB     14         18        100   \n",
       "2      yolov8l  43643718  Tesla V100-PCIE-32GB     14         18        100   \n",
       "3      yolov8l  43643718  Tesla V100-PCIE-32GB     14         18        100   \n",
       "4      yolov8l  43643718  Tesla V100-PCIE-32GB     14         18        100   \n",
       "..         ...       ...                   ...    ...        ...        ...   \n",
       "500    yolov8s  11142566  Tesla V100-PCIE-32GB     14         18        100   \n",
       "501    yolov8s  11142566  Tesla V100-PCIE-32GB     14         18        100   \n",
       "502    yolov8s  11142566  Tesla V100-PCIE-32GB     14         18        100   \n",
       "503    yolov8s  11142566  Tesla V100-PCIE-32GB     14         18        100   \n",
       "504    yolov8s  11142566  Tesla V100-PCIE-32GB     14         18        100   \n",
       "\n",
       "     t_instance_num  v_img_num  v_instance_num  imgsz  ...  preprocess_time  \\\n",
       "0               238        100             235    640  ...         0.000000   \n",
       "1               238        100             235    640  ...         0.004176   \n",
       "2               238        100             235    640  ...         0.008434   \n",
       "3               238        100             235    640  ...         0.009067   \n",
       "4               238        100             235    640  ...         0.009782   \n",
       "..              ...        ...             ...    ...  ...              ...   \n",
       "500             238        100             235    640  ...         0.003765   \n",
       "501             238        100             235    640  ...         0.003070   \n",
       "502             238        100             235    640  ...         0.003672   \n",
       "503             238        100             235    640  ...         0.005067   \n",
       "504             238        100             235    640  ...         0.003761   \n",
       "\n",
       "     train_time  val_time  save_model_time  scheduler_time  epoch_time  \\\n",
       "0      0.000000  0.000000         0.000000        0.000000    0.000000   \n",
       "1    114.019438  2.826854         4.204662        0.010752  121.066030   \n",
       "2    114.627127  2.597382         6.165627        0.015966  122.241334   \n",
       "3    112.422867  2.535480         6.449841        0.018609  121.973749   \n",
       "4    112.300142  2.535398         3.453643        0.010720  121.058286   \n",
       "..          ...       ...              ...             ...         ...   \n",
       "500    7.788323  0.606764         1.326586        0.025290   13.981678   \n",
       "501    7.735800  0.598938         1.533527        0.021495   13.939540   \n",
       "502    7.776962  0.607911         2.577138        0.018002   13.909393   \n",
       "503    7.781471  0.596876         1.531579        0.020663   13.869269   \n",
       "504    7.779656  0.599159         1.451818        0.022694   13.829164   \n",
       "\n",
       "         elapsed  remaining  gpu_usage  cpu_usage  \n",
       "0       0.000000          0   0.000000   0.000000  \n",
       "1     121.066030      11985   1.756200   1.131000  \n",
       "2     244.482668      11999   1.767350   1.206500  \n",
       "3     365.921246      11832   1.747733   1.204667  \n",
       "4     484.233146      11571   1.765100   1.186500  \n",
       "..           ...        ...        ...        ...  \n",
       "500  1342.241058         70   3.945610   6.341964  \n",
       "501  1352.135335         46   3.945596   6.340059  \n",
       "502  1363.120502         28   3.945758   6.334840  \n",
       "503  1373.057678         12   3.945859   6.327417  \n",
       "504  1382.916353          0   3.945771   6.326000  \n",
       "\n",
       "[1515 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/DATA/yolov8l.csv')\n",
    "df2 = pd.read_csv('/DATA/yolov8m.csv')\n",
    "df3 = pd.read_csv('/DATA/yolov8s.csv')\n",
    "\n",
    "df_all = pd.concat((df1, df2, df3))\n",
    "# df_all = df3\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터를 생성하기 위한 랜덤 설정\n",
    "np.random.seed(0)\n",
    "\n",
    "# 데이터셋의 크기 (행의 수)\n",
    "num_rows = 100\n",
    "\n",
    "# 임의의 데이터 생성\n",
    "data = {\n",
    "    \"GPU Type\": [\"Tesla V100\"] * num_rows,\n",
    "    \"Flops\": np.random.uniform(10, 15, num_rows),  # Tesla V100의 대략적인 Flops 범위\n",
    "    \"Image Num\": np.random.randint(1000, 10000, num_rows),\n",
    "    \"Input Image Size\": np.random.choice([320, 640, 720, 960, 1280], num_rows),\n",
    "    \"Batch Size\": np.random.choice([1, 2, 4, 8, 16, 32], num_rows),\n",
    "    \"Model Parameter Num\": np.random.randint(100000, 1000000, num_rows),\n",
    "    \"Instance Num\": np.random.randint(1, 10, num_rows),\n",
    "    \"1 Epoch Train Time\": np.random.uniform(10, 1000, num_rows)  # 초 단위\n",
    "}\n",
    "\n",
    "# Pandas DataFrame으로 변환\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# # CSV 파일로 저장\n",
    "# csv_file_path = '/mnt/data/object_detection_training_time_dataset_v2.csv'\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100\n",
    "\n",
    "data = {\n",
    "    \"GPUType\": [\"Tesla V100\"],\n",
    "    \"Flops\": np.random.uniform(10, 15),  # Tesla V100의 대략적인 Flops 범위\n",
    "    \"ImageNum\": 100,\n",
    "    \"InputImage Size\": 320,\n",
    "    \"BatchSize\": 2,\n",
    "    \"ModelParameterNum\": 1200000000,\n",
    "    \"1EpochTrainTime\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgnum = data[\"ImageNum\"]\n",
    "TT = data[\"1EpochTrainTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = TT/imgnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesla_V100', 9.746534794212451, 0, 320, 1, 1200000, 0.0]\n",
      "['Tesla_V100', 10.406354227367547, 1, 320, 1, 1200000, 10.016021382639266]\n",
      "['Tesla_V100', 10.097936730432469, 2, 320, 1, 1200000, 19.895217409455405]\n",
      "['Tesla_V100', 10.080887274147797, 3, 320, 1, 1200000, 29.78752209243678]\n",
      "['Tesla_V100', 9.835630978943882, 4, 320, 1, 1200000, 40.10042237685017]\n",
      "['Tesla_V100', 9.861305681589716, 5, 320, 1, 1200000, 50.410993987675]\n",
      "['Tesla_V100', 10.37849262386274, 6, 320, 1, 1200000, 59.92098669165323]\n",
      "['Tesla_V100', 10.017612100102593, 7, 320, 1, 1200000, 69.96453518324216]\n",
      "['Tesla_V100', 9.684362595825522, 8, 320, 1, 1200000, 79.8956170308359]\n",
      "['Tesla_V100', 9.68755193555789, 9, 320, 1, 1200000, 90.38737410122225]\n",
      "['Tesla_V100', 10.023006389323223, 10, 320, 1, 1200000, 99.64113065597358]\n",
      "['Tesla_V100', 9.503099541084387, 11, 320, 1, 1200000, 110.21406646695307]\n",
      "['Tesla_V100', 10.149078334705868, 12, 320, 1, 1200000, 118.81245510541547]\n",
      "['Tesla_V100', 10.470221803893583, 13, 320, 1, 1200000, 130.4260517765043]\n",
      "['Tesla_V100', 9.793144402506565, 14, 320, 1, 1200000, 139.16102010143535]\n",
      "['Tesla_V100', 10.425702151710162, 15, 320, 1, 1200000, 149.42775386894448]\n",
      "['Tesla_V100', 9.90321803304553, 16, 320, 1, 1200000, 160.42579842182192]\n",
      "['Tesla_V100', 10.390652809400207, 17, 320, 1, 1200000, 169.7708616141301]\n",
      "['Tesla_V100', 10.428218710843622, 18, 320, 1, 1200000, 180.33149038564352]\n",
      "['Tesla_V100', 9.931784927625163, 19, 320, 1, 1200000, 190.35256502857112]\n",
      "['Tesla_V100', 9.854506053751736, 20, 320, 1, 1200000, 200.628079096776]\n",
      "['Tesla_V100', 10.152308952741524, 21, 320, 1, 1200000, 211.35121796703743]\n",
      "['Tesla_V100', 9.902766759673185, 22, 320, 1, 1200000, 217.96528623101727]\n",
      "['Tesla_V100', 10.484631622969307, 23, 320, 1, 1200000, 229.76145702688746]\n",
      "['Tesla_V100', 10.437635225337615, 24, 320, 1, 1200000, 240.82579707430793]\n",
      "['Tesla_V100', 9.766696877600443, 25, 320, 1, 1200000, 250.32032180166965]\n",
      "['Tesla_V100', 10.456932975087776, 26, 320, 1, 1200000, 257.990126759044]\n",
      "['Tesla_V100', 9.772229361053542, 27, 320, 1, 1200000, 267.7431118249707]\n",
      "['Tesla_V100', 10.265270583798259, 28, 320, 1, 1200000, 277.3214568537075]\n",
      "['Tesla_V100', 9.637251350524139, 29, 320, 1, 1200000, 288.6116462679772]\n",
      "['Tesla_V100', 10.371082572438445, 30, 320, 1, 1200000, 298.07646041924096]\n",
      "['Tesla_V100', 9.933278732875888, 31, 320, 1, 1200000, 308.91573103713563]\n",
      "['Tesla_V100', 10.083404187781325, 32, 320, 1, 1200000, 322.79339584090224]\n",
      "['Tesla_V100', 10.44078165924661, 33, 320, 1, 1200000, 327.0465539051331]\n",
      "['Tesla_V100', 10.096211120362776, 34, 320, 1, 1200000, 342.66424656449743]\n",
      "['Tesla_V100', 10.28093113288533, 35, 320, 1, 1200000, 347.9807372840841]\n",
      "['Tesla_V100', 10.26499444756608, 36, 320, 1, 1200000, 357.7612511510304]\n",
      "['Tesla_V100', 10.398759587571492, 37, 320, 1, 1200000, 366.3660913610437]\n",
      "['Tesla_V100', 9.590232979489691, 38, 320, 1, 1200000, 380.9057892770784]\n",
      "['Tesla_V100', 9.882440099429765, 39, 320, 1, 1200000, 386.8531904527426]\n",
      "['Tesla_V100', 10.484933308210826, 40, 320, 1, 1200000, 403.17265004268495]\n",
      "['Tesla_V100', 9.771113391746736, 41, 320, 1, 1200000, 409.78015839122116]\n",
      "['Tesla_V100', 9.979753870197078, 42, 320, 1, 1200000, 420.0704792695618]\n",
      "['Tesla_V100', 10.467259305892187, 43, 320, 1, 1200000, 426.55964323409853]\n",
      "['Tesla_V100', 9.539650455991294, 44, 320, 1, 1200000, 439.8566168605314]\n",
      "['Tesla_V100', 9.958259011833317, 45, 320, 1, 1200000, 447.6173655956858]\n",
      "['Tesla_V100', 9.55887028330105, 46, 320, 1, 1200000, 456.53936864116343]\n",
      "['Tesla_V100', 10.356755247069648, 47, 320, 1, 1200000, 469.0568732977536]\n",
      "['Tesla_V100', 9.881397153430887, 48, 320, 1, 1200000, 475.41094690073203]\n",
      "['Tesla_V100', 9.874752704907415, 49, 320, 1, 1200000, 491.58375142579087]\n",
      "['Tesla_V100', 10.125358200240186, 50, 320, 1, 1200000, 504.13991217713215]\n",
      "['Tesla_V100', 10.171794300722647, 51, 320, 1, 1200000, 508.2056434532748]\n",
      "['Tesla_V100', 9.816219986308196, 52, 320, 1, 1200000, 523.4664843992321]\n",
      "['Tesla_V100', 10.275803218821984, 53, 320, 1, 1200000, 527.6802214399207]\n",
      "['Tesla_V100', 10.213927194699275, 54, 320, 1, 1200000, 540.6650346335944]\n",
      "['Tesla_V100', 9.899660014434042, 55, 320, 1, 1200000, 550.3394614358946]\n",
      "['Tesla_V100', 9.58523306029452, 56, 320, 1, 1200000, 561.8555905871809]\n",
      "['Tesla_V100', 9.529634818130301, 57, 320, 1, 1200000, 574.437875403297]\n",
      "['Tesla_V100', 10.182118958735222, 58, 320, 1, 1200000, 584.5392464633182]\n",
      "['Tesla_V100', 9.501207571736849, 59, 320, 1, 1200000, 595.1591821414983]\n",
      "['Tesla_V100', 10.458307359547792, 60, 320, 1, 1200000, 603.5558329551011]\n",
      "['Tesla_V100', 10.492073842812829, 61, 320, 1, 1200000, 608.2337474169827]\n",
      "['Tesla_V100', 10.34834015010409, 62, 320, 1, 1200000, 623.3563637468997]\n",
      "['Tesla_V100', 10.220527517596006, 63, 320, 1, 1200000, 631.6810267508632]\n",
      "['Tesla_V100', 9.807376117114217, 64, 320, 1, 1200000, 640.4771715245544]\n",
      "['Tesla_V100', 9.77457504884449, 65, 320, 1, 1200000, 643.6129689387991]\n",
      "['Tesla_V100', 9.524944724627135, 66, 320, 1, 1200000, 655.3045764749734]\n",
      "['Tesla_V100', 10.013759732735194, 67, 320, 1, 1200000, 666.2836162989892]\n",
      "['Tesla_V100', 9.869489693359437, 68, 320, 1, 1200000, 673.9494589717067]\n",
      "['Tesla_V100', 10.411197161476728, 69, 320, 1, 1200000, 695.3123361222331]\n",
      "['Tesla_V100', 10.397277073214717, 70, 320, 1, 1200000, 697.6506724267776]\n",
      "['Tesla_V100', 10.212926852948845, 71, 320, 1, 1200000, 708.9625964276562]\n",
      "['Tesla_V100', 9.801692466061118, 72, 320, 1, 1200000, 713.8139159683384]\n",
      "['Tesla_V100', 9.921844517819745, 73, 320, 1, 1200000, 728.3694662119246]\n",
      "['Tesla_V100', 9.660393536306564, 74, 320, 1, 1200000, 737.7830674622672]\n",
      "['Tesla_V100', 9.595619882587856, 75, 320, 1, 1200000, 755.5665379999249]\n",
      "['Tesla_V100', 10.167593305489307, 76, 320, 1, 1200000, 755.3293132469631]\n",
      "['Tesla_V100', 10.418091951007264, 77, 320, 1, 1200000, 771.2270857454388]\n",
      "['Tesla_V100', 9.955898437656932, 78, 320, 1, 1200000, 782.22068729007]\n",
      "['Tesla_V100', 9.754025231040357, 79, 320, 1, 1200000, 790.8070505464248]\n",
      "['Tesla_V100', 9.631903249740379, 80, 320, 1, 1200000, 795.2082562067199]\n",
      "['Tesla_V100', 10.014054471830578, 81, 320, 1, 1200000, 803.4793928242843]\n",
      "['Tesla_V100', 10.351853699366602, 82, 320, 1, 1200000, 818.2869188086797]\n",
      "['Tesla_V100', 9.733077824464488, 83, 320, 1, 1200000, 831.4181724407832]\n",
      "['Tesla_V100', 10.246836371748598, 84, 320, 1, 1200000, 841.831008718909]\n",
      "['Tesla_V100', 10.079011206424733, 85, 320, 1, 1200000, 844.3916938631481]\n",
      "['Tesla_V100', 10.18636204959375, 86, 320, 1, 1200000, 868.2009910134782]\n",
      "['Tesla_V100', 10.108593307185098, 87, 320, 1, 1200000, 867.0137937190597]\n",
      "['Tesla_V100', 10.302692139896777, 88, 320, 1, 1200000, 885.9200068836749]\n",
      "['Tesla_V100', 9.915279946705839, 89, 320, 1, 1200000, 888.4978270446476]\n",
      "['Tesla_V100', 10.436617108190113, 90, 320, 1, 1200000, 903.9993124290793]\n",
      "['Tesla_V100', 9.532901334553351, 91, 320, 1, 1200000, 908.6522162007182]\n",
      "['Tesla_V100', 9.593894468603171, 92, 320, 1, 1200000, 928.8052748401127]\n",
      "['Tesla_V100', 10.075212470134925, 93, 320, 1, 1200000, 932.6724124262345]\n",
      "['Tesla_V100', 9.864462684427107, 94, 320, 1, 1200000, 943.1530399107838]\n",
      "['Tesla_V100', 10.071843638127135, 95, 320, 1, 1200000, 955.2189860266112]\n",
      "['Tesla_V100', 10.400102131008286, 96, 320, 1, 1200000, 958.1626853051292]\n",
      "['Tesla_V100', 10.395345113919284, 97, 320, 1, 1200000, 961.562600139994]\n",
      "['Tesla_V100', 10.185477381018469, 98, 320, 1, 1200000, 974.7289729982602]\n",
      "['Tesla_V100', 10.03886850339515, 99, 320, 1, 1200000, 995.4060925281625]\n",
      "['Tesla_V100', 10.246584288200413, 100, 320, 1, 1200000, 1004.4286022028441]\n",
      "['Tesla_V100', 9.98289162258129, 101, 320, 1, 1200000, 1003.8192608757164]\n",
      "['Tesla_V100', 9.705566322640863, 102, 320, 1, 1200000, 1020.7235524387836]\n",
      "['Tesla_V100', 9.937814251624589, 103, 320, 1, 1200000, 1039.784790132897]\n",
      "['Tesla_V100', 9.800823925754838, 104, 320, 1, 1200000, 1031.24257411521]\n",
      "['Tesla_V100', 10.226877965796096, 105, 320, 1, 1200000, 1059.9839068225328]\n",
      "['Tesla_V100', 10.282089377225926, 106, 320, 1, 1200000, 1054.6373584955752]\n",
      "['Tesla_V100', 9.53656098562973, 107, 320, 1, 1200000, 1068.008129332237]\n",
      "['Tesla_V100', 10.377750788392257, 108, 320, 1, 1200000, 1078.5310657421173]\n",
      "['Tesla_V100', 9.784086135003218, 109, 320, 1, 1200000, 1099.489581112281]\n",
      "['Tesla_V100', 10.27226558309707, 110, 320, 1, 1200000, 1102.441703174385]\n",
      "['Tesla_V100', 9.502677071033395, 111, 320, 1, 1200000, 1119.5794117478756]\n",
      "['Tesla_V100', 10.17383734323381, 112, 320, 1, 1200000, 1113.8280594201983]\n",
      "['Tesla_V100', 9.52475307911629, 113, 320, 1, 1200000, 1118.8687839810352]\n",
      "['Tesla_V100', 10.427402551534048, 114, 320, 1, 1200000, 1144.4038839087252]\n",
      "['Tesla_V100', 10.20514754063908, 115, 320, 1, 1200000, 1152.2094947718665]\n",
      "['Tesla_V100', 10.037235654550297, 116, 320, 1, 1200000, 1158.9694226098427]\n",
      "['Tesla_V100', 10.10019320918279, 117, 320, 1, 1200000, 1173.6055171967837]\n",
      "['Tesla_V100', 9.873039777259713, 118, 320, 1, 1200000, 1183.970321761603]\n",
      "['Tesla_V100', 9.956884647075608, 119, 320, 1, 1200000, 1180.1245900616848]\n",
      "['Tesla_V100', 9.733173751178587, 120, 320, 1, 1200000, 1189.9529212497057]\n",
      "['Tesla_V100', 9.822017335765311, 121, 320, 1, 1200000, 1212.6941235687825]\n",
      "['Tesla_V100', 9.867626173386224, 122, 320, 1, 1200000, 1225.5199432675283]\n",
      "['Tesla_V100', 10.178059410212013, 123, 320, 1, 1200000, 1221.2594020308065]\n",
      "['Tesla_V100', 9.66804252136798, 124, 320, 1, 1200000, 1229.1979990420948]\n",
      "['Tesla_V100', 9.764332757951715, 125, 320, 1, 1200000, 1246.2089188271004]\n",
      "['Tesla_V100', 9.588248239572334, 126, 320, 1, 1200000, 1262.576175257788]\n",
      "['Tesla_V100', 10.214375740311246, 127, 320, 1, 1200000, 1261.293441360014]\n",
      "['Tesla_V100', 9.842137512607216, 128, 320, 1, 1200000, 1285.8175976020168]\n",
      "['Tesla_V100', 10.163317924200655, 129, 320, 1, 1200000, 1299.1450687248]\n",
      "['Tesla_V100', 10.054603388994474, 130, 320, 1, 1200000, 1287.669667947769]\n",
      "['Tesla_V100', 9.5573206239556, 131, 320, 1, 1200000, 1298.9976006455195]\n",
      "['Tesla_V100', 10.01713727632827, 132, 320, 1, 1200000, 1326.7348031198783]\n",
      "['Tesla_V100', 9.53227328820608, 133, 320, 1, 1200000, 1328.8478899806776]\n",
      "['Tesla_V100', 10.324797705614646, 134, 320, 1, 1200000, 1344.5186367206625]\n",
      "['Tesla_V100', 10.045917596467904, 135, 320, 1, 1200000, 1352.3979161591951]\n",
      "['Tesla_V100', 10.39647999952204, 136, 320, 1, 1200000, 1362.0421164985341]\n",
      "['Tesla_V100', 9.921819220285945, 137, 320, 1, 1200000, 1375.560876875583]\n",
      "['Tesla_V100', 9.857207424112111, 138, 320, 1, 1200000, 1393.0523641165264]\n",
      "['Tesla_V100', 10.372302509882696, 139, 320, 1, 1200000, 1380.4549670599222]\n",
      "['Tesla_V100', 10.184125728305485, 140, 320, 1, 1200000, 1386.9539814285395]\n",
      "['Tesla_V100', 10.38520727438484, 141, 320, 1, 1200000, 1419.7248710575343]\n",
      "['Tesla_V100', 10.010127999933928, 142, 320, 1, 1200000, 1409.426357419755]\n",
      "['Tesla_V100', 10.121473036237933, 143, 320, 1, 1200000, 1422.5450383186826]\n",
      "['Tesla_V100', 10.130871067068256, 144, 320, 1, 1200000, 1445.579876519601]\n",
      "['Tesla_V100', 9.606254909934199, 145, 320, 1, 1200000, 1449.3314308593624]\n",
      "['Tesla_V100', 10.085593381135176, 146, 320, 1, 1200000, 1452.8058706663696]\n",
      "['Tesla_V100', 9.701010285865939, 147, 320, 1, 1200000, 1474.0529590357962]\n",
      "['Tesla_V100', 10.260743565104612, 148, 320, 1, 1200000, 1475.829675381566]\n",
      "['Tesla_V100', 10.208897557352994, 149, 320, 1, 1200000, 1492.6037841791126]\n",
      "['Tesla_V100', 10.309593381855954, 150, 320, 1, 1200000, 1491.12216184591]\n",
      "['Tesla_V100', 10.296396765359322, 151, 320, 1, 1200000, 1502.9455186470739]\n",
      "['Tesla_V100', 9.798264420889101, 152, 320, 1, 1200000, 1534.5364340770618]\n",
      "['Tesla_V100', 9.546013576742611, 153, 320, 1, 1200000, 1520.506460665698]\n",
      "['Tesla_V100', 10.020874795151572, 154, 320, 1, 1200000, 1536.180623543253]\n",
      "['Tesla_V100', 9.742099708285856, 155, 320, 1, 1200000, 1555.721134913567]\n",
      "['Tesla_V100', 10.169236963770336, 156, 320, 1, 1200000, 1574.468719045519]\n",
      "['Tesla_V100', 10.378600374918479, 157, 320, 1, 1200000, 1585.6194944053195]\n",
      "['Tesla_V100', 9.862887996743034, 158, 320, 1, 1200000, 1565.6294098866774]\n",
      "['Tesla_V100', 9.825296807649503, 159, 320, 1, 1200000, 1589.9018890057318]\n",
      "['Tesla_V100', 9.672195707419286, 160, 320, 1, 1200000, 1586.722434401078]\n",
      "['Tesla_V100', 9.95272885908774, 161, 320, 1, 1200000, 1623.9318301157136]\n",
      "['Tesla_V100', 10.263165007542282, 162, 320, 1, 1200000, 1613.8921486191512]\n",
      "['Tesla_V100', 10.388615495047262, 163, 320, 1, 1200000, 1636.4311419393946]\n",
      "['Tesla_V100', 9.963344016756526, 164, 320, 1, 1200000, 1632.279520573823]\n",
      "['Tesla_V100', 10.075533888310394, 165, 320, 1, 1200000, 1639.931733126283]\n",
      "['Tesla_V100', 10.241231665910185, 166, 320, 1, 1200000, 1674.5060056113043]\n",
      "['Tesla_V100', 10.159267866427989, 167, 320, 1, 1200000, 1677.5148027410432]\n",
      "['Tesla_V100', 9.664488411693773, 168, 320, 1, 1200000, 1688.3403253340057]\n",
      "['Tesla_V100', 9.997356839332623, 169, 320, 1, 1200000, 1676.0535349164907]\n",
      "['Tesla_V100', 9.886447406288017, 170, 320, 1, 1200000, 1707.8012358789235]\n",
      "['Tesla_V100', 10.078210349141937, 171, 320, 1, 1200000, 1703.378286670954]\n",
      "['Tesla_V100', 10.488749003092677, 172, 320, 1, 1200000, 1722.5466952832141]\n",
      "['Tesla_V100', 10.44494919223576, 173, 320, 1, 1200000, 1732.068188402798]\n",
      "['Tesla_V100', 10.064666535593313, 174, 320, 1, 1200000, 1724.3807312351498]\n",
      "['Tesla_V100', 9.604078766365685, 175, 320, 1, 1200000, 1754.005332208724]\n",
      "['Tesla_V100', 10.231633713286918, 176, 320, 1, 1200000, 1750.9899414640458]\n",
      "['Tesla_V100', 10.463975725867675, 177, 320, 1, 1200000, 1778.5398278242142]\n",
      "['Tesla_V100', 9.738011277490479, 178, 320, 1, 1200000, 1785.1413620168573]\n",
      "['Tesla_V100', 9.83975637952292, 179, 320, 1, 1200000, 1798.143810495514]\n",
      "['Tesla_V100', 10.084592576888122, 180, 320, 1, 1200000, 1809.2395338231386]\n",
      "['Tesla_V100', 9.751463787989223, 181, 320, 1, 1200000, 1825.6490901728262]\n",
      "['Tesla_V100', 10.493001314478121, 182, 320, 1, 1200000, 1807.7889909954758]\n",
      "['Tesla_V100', 10.012905323061227, 183, 320, 1, 1200000, 1843.3032295966257]\n",
      "['Tesla_V100', 9.847697988665491, 184, 320, 1, 1200000, 1830.5290536192815]\n",
      "['Tesla_V100', 10.231065209174403, 185, 320, 1, 1200000, 1867.6681594587942]\n",
      "['Tesla_V100', 10.003747818757791, 186, 320, 1, 1200000, 1855.9984171576675]\n",
      "['Tesla_V100', 9.891932865439141, 187, 320, 1, 1200000, 1881.014541568521]\n",
      "['Tesla_V100', 10.20606100721406, 188, 320, 1, 1200000, 1873.191291473922]\n",
      "['Tesla_V100', 10.474588656145022, 189, 320, 1, 1200000, 1904.8017424076334]\n",
      "['Tesla_V100', 10.200928063540125, 190, 320, 1, 1200000, 1897.7495333189459]\n",
      "['Tesla_V100', 9.707295980155921, 191, 320, 1, 1200000, 1916.301651621961]\n",
      "['Tesla_V100', 10.253890066248223, 192, 320, 1, 1200000, 1923.9965778034539]\n",
      "['Tesla_V100', 9.995616780371247, 193, 320, 1, 1200000, 1915.2564043972172]\n",
      "['Tesla_V100', 10.284730569358885, 194, 320, 1, 1200000, 1939.6737473741773]\n",
      "['Tesla_V100', 10.354114885049563, 195, 320, 1, 1200000, 1935.757927181896]\n",
      "['Tesla_V100', 10.465089384336652, 196, 320, 1, 1200000, 1959.6550808117481]\n",
      "['Tesla_V100', 9.57235036841916, 197, 320, 1, 1200000, 1961.33648168325]\n",
      "['Tesla_V100', 10.402580207100613, 198, 320, 1, 1200000, 1973.4271107657953]\n",
      "['Tesla_V100', 10.470078102791017, 199, 320, 1, 1200000, 1987.7760320883335]\n",
      "['Tesla_V100', 9.867046244441037, 200, 320, 1, 1200000, 2011.5907845488964]\n",
      "['Tesla_V100', 9.967119407168838, 201, 320, 1, 1200000, 1992.9377758001385]\n",
      "['Tesla_V100', 10.328840906204086, 202, 320, 1, 1200000, 2031.734676503234]\n",
      "['Tesla_V100', 10.01499291451066, 203, 320, 1, 1200000, 2010.093142669601]\n",
      "['Tesla_V100', 9.63635700735406, 204, 320, 1, 1200000, 2048.6492155284755]\n",
      "['Tesla_V100', 10.090857549922902, 205, 320, 1, 1200000, 2058.2018245621534]\n",
      "['Tesla_V100', 9.860934472634138, 206, 320, 1, 1200000, 2065.3303203788405]\n",
      "['Tesla_V100', 9.934826636417473, 207, 320, 1, 1200000, 2066.431362235015]\n",
      "['Tesla_V100', 9.915032617838467, 208, 320, 1, 1200000, 2060.5335137414168]\n",
      "['Tesla_V100', 9.560995006781575, 209, 320, 1, 1200000, 2104.6015410452105]\n",
      "['Tesla_V100', 10.195525539940991, 210, 320, 1, 1200000, 2088.677291468549]\n",
      "['Tesla_V100', 9.618126701674775, 211, 320, 1, 1200000, 2090.6179730773047]\n",
      "['Tesla_V100', 9.979707153354932, 212, 320, 1, 1200000, 2123.729806106618]\n",
      "['Tesla_V100', 10.39795593614042, 213, 320, 1, 1200000, 2147.903576734876]\n",
      "['Tesla_V100', 9.866244319646256, 214, 320, 1, 1200000, 2158.4399286503744]\n",
      "['Tesla_V100', 10.0457793402968, 215, 320, 1, 1200000, 2153.185951507236]\n",
      "['Tesla_V100', 9.678635638356816, 216, 320, 1, 1200000, 2143.4052147844773]\n",
      "['Tesla_V100', 9.744376072469455, 217, 320, 1, 1200000, 2153.694137994867]\n",
      "['Tesla_V100', 9.723291669654248, 218, 320, 1, 1200000, 2181.735863270505]\n",
      "['Tesla_V100', 10.368254984077398, 219, 320, 1, 1200000, 2202.9204650372535]\n",
      "['Tesla_V100', 9.5323562325608, 220, 320, 1, 1200000, 2208.7365260281495]\n",
      "['Tesla_V100', 9.546791788112063, 221, 320, 1, 1200000, 2232.01272648998]\n",
      "['Tesla_V100', 10.284114350293867, 222, 320, 1, 1200000, 2208.073351012497]\n",
      "['Tesla_V100', 9.990110264985518, 223, 320, 1, 1200000, 2240.8869149004986]\n",
      "['Tesla_V100', 10.339806524214314, 224, 320, 1, 1200000, 2227.852811080922]\n",
      "['Tesla_V100', 9.633948890702994, 225, 320, 1, 1200000, 2269.398446665522]\n",
      "['Tesla_V100', 10.170761655944734, 226, 320, 1, 1200000, 2277.0080770848526]\n",
      "['Tesla_V100', 9.73238315430146, 227, 320, 1, 1200000, 2253.1428262745794]\n",
      "['Tesla_V100', 9.850325185592899, 228, 320, 1, 1200000, 2268.723117011078]\n",
      "['Tesla_V100', 9.57275408464559, 229, 320, 1, 1200000, 2296.6171429231285]\n",
      "['Tesla_V100', 9.751211961503245, 230, 320, 1, 1200000, 2297.710325584558]\n",
      "['Tesla_V100', 9.78830055336939, 231, 320, 1, 1200000, 2329.459080028959]\n",
      "['Tesla_V100', 9.950209164077744, 232, 320, 1, 1200000, 2337.6253502875384]\n",
      "['Tesla_V100', 9.544016362293194, 233, 320, 1, 1200000, 2331.668369456825]\n",
      "['Tesla_V100', 9.95273049642672, 234, 320, 1, 1200000, 2331.862308001884]\n",
      "['Tesla_V100', 9.800189458014612, 235, 320, 1, 1200000, 2339.039043428119]\n",
      "['Tesla_V100', 9.699529533476566, 236, 320, 1, 1200000, 2359.652250822434]\n",
      "['Tesla_V100', 9.845137771035171, 237, 320, 1, 1200000, 2361.9704651068178]\n",
      "['Tesla_V100', 9.566207172465568, 238, 320, 1, 1200000, 2356.8599762040603]\n",
      "['Tesla_V100', 10.411656755832977, 239, 320, 1, 1200000, 2385.8776528972553]\n",
      "['Tesla_V100', 10.177044539330286, 240, 320, 1, 1200000, 2397.7151503203295]\n",
      "['Tesla_V100', 9.700637670685568, 241, 320, 1, 1200000, 2430.700703998932]\n",
      "['Tesla_V100', 9.622661983334595, 242, 320, 1, 1200000, 2419.098561085833]\n",
      "['Tesla_V100', 9.5917280964922, 243, 320, 1, 1200000, 2416.8428181674017]\n",
      "['Tesla_V100', 10.185414932707772, 244, 320, 1, 1200000, 2441.2889319163123]\n",
      "['Tesla_V100', 10.050054509276315, 245, 320, 1, 1200000, 2466.279704191396]\n",
      "['Tesla_V100', 9.560778156303432, 246, 320, 1, 1200000, 2479.9174740277485]\n",
      "['Tesla_V100', 10.151227562569101, 247, 320, 1, 1200000, 2476.9608219035727]\n",
      "['Tesla_V100', 9.530208200221242, 248, 320, 1, 1200000, 2500.3396017931905]\n",
      "['Tesla_V100', 9.52798199115936, 249, 320, 1, 1200000, 2474.3353089023954]\n",
      "['Tesla_V100', 10.395911192085654, 250, 320, 1, 1200000, 2481.3363835621267]\n",
      "['Tesla_V100', 9.897988157455993, 251, 320, 1, 1200000, 2523.0544699083994]\n",
      "['Tesla_V100', 10.062947441931536, 252, 320, 1, 1200000, 2531.6160938310786]\n",
      "['Tesla_V100', 9.692846100016823, 253, 320, 1, 1200000, 2525.549853637738]\n",
      "['Tesla_V100', 9.998388097740392, 254, 320, 1, 1200000, 2526.9760750527735]\n",
      "['Tesla_V100', 10.497917524435543, 255, 320, 1, 1200000, 2558.726894292984]\n",
      "['Tesla_V100', 10.383779436669546, 256, 320, 1, 1200000, 2554.519898872702]\n",
      "['Tesla_V100', 10.424839816341432, 257, 320, 1, 1200000, 2557.233615269983]\n",
      "['Tesla_V100', 9.657883870260603, 258, 320, 1, 1200000, 2594.8019774094696]\n",
      "['Tesla_V100', 10.473016632757345, 259, 320, 1, 1200000, 2567.313455503018]\n",
      "['Tesla_V100', 9.600002676995484, 260, 320, 1, 1200000, 2590.53068920573]\n",
      "['Tesla_V100', 10.025950938590322, 261, 320, 1, 1200000, 2620.6629615432043]\n",
      "['Tesla_V100', 10.455550548546448, 262, 320, 1, 1200000, 2599.9737542729663]\n",
      "['Tesla_V100', 9.856314981698617, 263, 320, 1, 1200000, 2617.706537791198]\n",
      "['Tesla_V100', 10.307418527200115, 264, 320, 1, 1200000, 2622.2359043637375]\n",
      "['Tesla_V100', 10.376839570590173, 265, 320, 1, 1200000, 2664.5066515022518]\n",
      "['Tesla_V100', 10.383818637721165, 266, 320, 1, 1200000, 2684.9597245418663]\n",
      "['Tesla_V100', 10.07344393784116, 267, 320, 1, 1200000, 2667.7792239994023]\n",
      "['Tesla_V100', 9.985030533394188, 268, 320, 1, 1200000, 2701.1994932733796]\n",
      "['Tesla_V100', 9.708683329505904, 269, 320, 1, 1200000, 2683.293853232311]\n",
      "['Tesla_V100', 10.141132931719527, 270, 320, 1, 1200000, 2711.66576302194]\n",
      "['Tesla_V100', 9.556372162119018, 271, 320, 1, 1200000, 2735.9159324728375]\n",
      "['Tesla_V100', 9.831451921956475, 272, 320, 1, 1200000, 2704.0098564357327]\n",
      "['Tesla_V100', 9.80717295074896, 273, 320, 1, 1200000, 2756.4685769968387]\n",
      "['Tesla_V100', 10.168577156680787, 274, 320, 1, 1200000, 2726.0412312243625]\n",
      "['Tesla_V100', 9.701096017888425, 275, 320, 1, 1200000, 2726.6192341215137]\n",
      "['Tesla_V100', 9.863213978617557, 276, 320, 1, 1200000, 2777.871174446519]\n",
      "['Tesla_V100', 9.775425263703024, 277, 320, 1, 1200000, 2762.7143992432125]\n",
      "['Tesla_V100', 10.261436693385726, 278, 320, 1, 1200000, 2790.8946453501244]\n",
      "['Tesla_V100', 9.912433549434864, 279, 320, 1, 1200000, 2771.269017862071]\n",
      "['Tesla_V100', 10.393772755849287, 280, 320, 1, 1200000, 2804.203455803753]\n",
      "['Tesla_V100', 9.756767816452845, 281, 320, 1, 1200000, 2795.2076000582056]\n",
      "['Tesla_V100', 9.948385673237228, 282, 320, 1, 1200000, 2814.5375010539246]\n",
      "['Tesla_V100', 9.568880454511683, 283, 320, 1, 1200000, 2855.0818620682153]\n",
      "['Tesla_V100', 10.286954143801466, 284, 320, 1, 1200000, 2839.803933713106]\n",
      "['Tesla_V100', 9.826902725565779, 285, 320, 1, 1200000, 2831.844150257338]\n",
      "['Tesla_V100', 9.666289570008589, 286, 320, 1, 1200000, 2845.156050629637]\n",
      "['Tesla_V100', 9.720528299733234, 287, 320, 1, 1200000, 2882.0368108536895]\n",
      "['Tesla_V100', 10.269633762721547, 288, 320, 1, 1200000, 2872.136549649519]\n",
      "['Tesla_V100', 9.659393707145536, 289, 320, 1, 1200000, 2898.3656448478355]\n",
      "['Tesla_V100', 9.832830986382481, 290, 320, 1, 1200000, 2925.07045377361]\n",
      "['Tesla_V100', 10.288660960767793, 291, 320, 1, 1200000, 2938.387538258136]\n",
      "['Tesla_V100', 10.387363062083653, 292, 320, 1, 1200000, 2891.110398353819]\n",
      "['Tesla_V100', 9.56033534371132, 293, 320, 1, 1200000, 2953.3340663100803]\n",
      "['Tesla_V100', 9.618932272687573, 294, 320, 1, 1200000, 2945.620318425381]\n",
      "['Tesla_V100', 9.854821131405075, 295, 320, 1, 1200000, 2922.492522463707]\n",
      "['Tesla_V100', 9.582876172591638, 296, 320, 1, 1200000, 2975.7372827901204]\n",
      "['Tesla_V100', 10.342401815941106, 297, 320, 1, 1200000, 2961.87550333778]\n",
      "['Tesla_V100', 9.7467724032637, 298, 320, 1, 1200000, 2979.6179999794726]\n",
      "['Tesla_V100', 9.65228310589695, 299, 320, 1, 1200000, 2996.993042509045]\n",
      "['Tesla_V100', 10.239507771908148, 300, 320, 1, 1200000, 3012.237934334873]\n",
      "['Tesla_V100', 10.432079638462875, 301, 320, 1, 1200000, 3016.123156013182]\n",
      "['Tesla_V100', 10.095197423879993, 302, 320, 1, 1200000, 3034.313908339941]\n",
      "['Tesla_V100', 10.225783711077764, 303, 320, 1, 1200000, 3016.617807561747]\n",
      "['Tesla_V100', 10.297253926580144, 304, 320, 1, 1200000, 3064.589937282515]\n",
      "['Tesla_V100', 10.279408732049099, 305, 320, 1, 1200000, 3071.312360684693]\n",
      "['Tesla_V100', 10.348721531829773, 306, 320, 1, 1200000, 3044.5542089592777]\n",
      "['Tesla_V100', 10.327051028072656, 307, 320, 1, 1200000, 3083.4454396085835]\n",
      "['Tesla_V100', 9.823365478337934, 308, 320, 1, 1200000, 3090.7604615699156]\n",
      "['Tesla_V100', 10.17319052872313, 309, 320, 1, 1200000, 3108.6855856063708]\n",
      "['Tesla_V100', 9.500060568600102, 310, 320, 1, 1200000, 3109.127011140793]\n",
      "['Tesla_V100', 10.172889163108305, 311, 320, 1, 1200000, 3114.815101255728]\n",
      "['Tesla_V100', 10.27737785890298, 312, 320, 1, 1200000, 3097.756182689305]\n",
      "['Tesla_V100', 9.801308924768328, 313, 320, 1, 1200000, 3146.0173075629828]\n",
      "['Tesla_V100', 9.842920794250556, 314, 320, 1, 1200000, 3146.5091836692955]\n",
      "['Tesla_V100', 10.12139259020239, 315, 320, 1, 1200000, 3137.3126607439685]\n",
      "['Tesla_V100', 9.960909393048654, 316, 320, 1, 1200000, 3171.6793066406417]\n",
      "['Tesla_V100', 10.273659043748754, 317, 320, 1, 1200000, 3179.3336194618655]\n",
      "['Tesla_V100', 9.885739066013544, 318, 320, 1, 1200000, 3150.208144437703]\n",
      "['Tesla_V100', 10.401007911619274, 319, 320, 1, 1200000, 3158.6981363375057]\n",
      "['Tesla_V100', 9.602389951252645, 320, 320, 1, 1200000, 3218.129547835566]\n",
      "['Tesla_V100', 9.560892225577083, 321, 320, 1, 1200000, 3234.7681249911034]\n",
      "['Tesla_V100', 10.398110969165408, 322, 320, 1, 1200000, 3191.4052046787697]\n",
      "['Tesla_V100', 10.450955860190087, 323, 320, 1, 1200000, 3230.9342751372847]\n",
      "['Tesla_V100', 9.50069155538579, 324, 320, 1, 1200000, 3241.5140311933033]\n",
      "['Tesla_V100', 9.88059537811768, 325, 320, 1, 1200000, 3281.54838972462]\n",
      "['Tesla_V100', 9.760588254468235, 326, 320, 1, 1200000, 3256.514847384266]\n",
      "['Tesla_V100', 9.594697231081284, 327, 320, 1, 1200000, 3294.632805878198]\n",
      "['Tesla_V100', 9.53753435005111, 328, 320, 1, 1200000, 3297.7934214984675]\n",
      "['Tesla_V100', 9.77832880703031, 329, 320, 1, 1200000, 3266.254970131313]\n",
      "['Tesla_V100', 10.201440751587263, 330, 320, 1, 1200000, 3308.21683344275]\n",
      "['Tesla_V100', 9.535581802310029, 331, 320, 1, 1200000, 3302.5967932953818]\n",
      "['Tesla_V100', 10.05567903880622, 332, 320, 1, 1200000, 3310.467608625908]\n",
      "['Tesla_V100', 10.131718525872502, 333, 320, 1, 1200000, 3327.5080103820283]\n",
      "['Tesla_V100', 10.146329186596684, 334, 320, 1, 1200000, 3347.317052627371]\n",
      "['Tesla_V100', 9.70095056150453, 335, 320, 1, 1200000, 3339.1353380849155]\n",
      "['Tesla_V100', 9.803252820441774, 336, 320, 1, 1200000, 3392.2835383833335]\n",
      "['Tesla_V100', 9.743903800634765, 337, 320, 1, 1200000, 3351.8225964287158]\n",
      "['Tesla_V100', 9.557470632999507, 338, 320, 1, 1200000, 3410.31475382997]\n",
      "['Tesla_V100', 9.976003158059967, 339, 320, 1, 1200000, 3423.1563786907536]\n",
      "['Tesla_V100', 10.249701444792121, 340, 320, 1, 1200000, 3368.3287218219516]\n",
      "['Tesla_V100', 10.295545387398743, 341, 320, 1, 1200000, 3437.87562066245]\n",
      "['Tesla_V100', 10.235740085644625, 342, 320, 1, 1200000, 3403.989645384831]\n",
      "['Tesla_V100', 9.645435603405367, 343, 320, 1, 1200000, 3409.012384881169]\n",
      "['Tesla_V100', 10.492886688261294, 344, 320, 1, 1200000, 3440.0014724810744]\n",
      "['Tesla_V100', 10.496540866457273, 345, 320, 1, 1200000, 3460.943680856904]\n",
      "['Tesla_V100', 10.43042838470333, 346, 320, 1, 1200000, 3469.924602547657]\n",
      "['Tesla_V100', 9.746577053425467, 347, 320, 1, 1200000, 3457.1625337952337]\n",
      "['Tesla_V100', 10.402978855594968, 348, 320, 1, 1200000, 3509.3777342862463]\n",
      "['Tesla_V100', 10.137840299085736, 349, 320, 1, 1200000, 3461.933140696935]\n",
      "['Tesla_V100', 9.76091105331621, 350, 320, 1, 1200000, 3477.455077784967]\n",
      "['Tesla_V100', 10.120053580355401, 351, 320, 1, 1200000, 3485.738149900032]\n",
      "['Tesla_V100', 9.940847499674549, 352, 320, 1, 1200000, 3523.691800596109]\n",
      "['Tesla_V100', 9.727870713720666, 353, 320, 1, 1200000, 3510.7372553934447]\n",
      "['Tesla_V100', 9.766945801834455, 354, 320, 1, 1200000, 3568.111669716567]\n",
      "['Tesla_V100', 10.064627209992539, 355, 320, 1, 1200000, 3523.432592323365]\n",
      "['Tesla_V100', 9.705011413542522, 356, 320, 1, 1200000, 3591.1033416892474]\n",
      "['Tesla_V100', 9.873105883978702, 357, 320, 1, 1200000, 3559.3176330078586]\n",
      "['Tesla_V100', 9.96999369800334, 358, 320, 1, 1200000, 3572.4219185634806]\n",
      "['Tesla_V100', 9.838281391897455, 359, 320, 1, 1200000, 3580.413259412963]\n",
      "['Tesla_V100', 10.366458669489392, 360, 320, 1, 1200000, 3614.0204235600995]\n",
      "['Tesla_V100', 9.517768678520145, 361, 320, 1, 1200000, 3643.078120726352]\n",
      "['Tesla_V100', 10.323571942717368, 362, 320, 1, 1200000, 3650.4847146276898]\n",
      "['Tesla_V100', 9.74069598218139, 363, 320, 1, 1200000, 3643.6824087274354]\n",
      "['Tesla_V100', 10.476761955286216, 364, 320, 1, 1200000, 3612.8519408666284]\n",
      "['Tesla_V100', 10.24668811766437, 365, 320, 1, 1200000, 3631.2047815729984]\n",
      "['Tesla_V100', 10.010020201065691, 366, 320, 1, 1200000, 3641.8810381379703]\n",
      "['Tesla_V100', 10.149031593534055, 367, 320, 1, 1200000, 3683.769054142058]\n",
      "['Tesla_V100', 9.54261593305968, 368, 320, 1, 1200000, 3683.623862272164]\n",
      "['Tesla_V100', 10.143198776216975, 369, 320, 1, 1200000, 3695.197247701561]\n",
      "['Tesla_V100', 9.773677548929333, 370, 320, 1, 1200000, 3679.340997024172]\n",
      "['Tesla_V100', 9.653582512461227, 371, 320, 1, 1200000, 3729.282594477533]\n",
      "['Tesla_V100', 10.292718364110199, 372, 320, 1, 1200000, 3723.7451272455673]\n",
      "['Tesla_V100', 10.032325622549429, 373, 320, 1, 1200000, 3719.6385432554025]\n",
      "['Tesla_V100', 9.575850091980781, 374, 320, 1, 1200000, 3776.2054665475293]\n",
      "['Tesla_V100', 10.369848071213601, 375, 320, 1, 1200000, 3727.9124344882375]\n",
      "['Tesla_V100', 9.924852639144753, 376, 320, 1, 1200000, 3771.46907885147]\n",
      "['Tesla_V100', 10.05264586988517, 377, 320, 1, 1200000, 3751.2404887859643]\n",
      "['Tesla_V100', 9.824052321203402, 378, 320, 1, 1200000, 3801.785244692742]\n",
      "['Tesla_V100', 9.662450730669832, 379, 320, 1, 1200000, 3758.283847668424]\n",
      "['Tesla_V100', 10.013184716298218, 380, 320, 1, 1200000, 3778.8210611229156]\n",
      "['Tesla_V100', 9.732316499711533, 381, 320, 1, 1200000, 3811.9748479620507]\n",
      "['Tesla_V100', 9.998533548694276, 382, 320, 1, 1200000, 3852.6960189962965]\n",
      "['Tesla_V100', 10.00344927801236, 383, 320, 1, 1200000, 3806.742259479878]\n",
      "['Tesla_V100', 9.527078231447852, 384, 320, 1, 1200000, 3808.064876718109]\n",
      "['Tesla_V100', 9.919857648716027, 385, 320, 1, 1200000, 3851.318346292912]\n",
      "['Tesla_V100', 10.146490590042815, 386, 320, 1, 1200000, 3857.7771622335736]\n",
      "['Tesla_V100', 10.158199246330412, 387, 320, 1, 1200000, 3831.3554926717716]\n",
      "['Tesla_V100', 9.514780925928742, 388, 320, 1, 1200000, 3903.4921315933425]\n",
      "['Tesla_V100', 10.11370212849892, 389, 320, 1, 1200000, 3909.8964149433486]\n",
      "['Tesla_V100', 9.87840336991541, 390, 320, 1, 1200000, 3907.872827543852]\n",
      "['Tesla_V100', 9.969169848250196, 391, 320, 1, 1200000, 3927.619729127248]\n",
      "['Tesla_V100', 9.5976596531763, 392, 320, 1, 1200000, 3890.011340901202]\n",
      "['Tesla_V100', 10.313223546057808, 393, 320, 1, 1200000, 3898.9822792165182]\n",
      "['Tesla_V100', 10.4645108154949, 394, 320, 1, 1200000, 3956.755989649911]\n",
      "['Tesla_V100', 10.023994479651396, 395, 320, 1, 1200000, 3980.503323200576]\n",
      "['Tesla_V100', 10.363330118036796, 396, 320, 1, 1200000, 3981.8138066620886]\n",
      "['Tesla_V100', 10.297292864272606, 397, 320, 1, 1200000, 3992.3503470584874]\n",
      "['Tesla_V100', 10.06970604196368, 398, 320, 1, 1200000, 3998.057341948751]\n",
      "['Tesla_V100', 9.957149624407533, 399, 320, 1, 1200000, 4014.1168190533517]\n",
      "['Tesla_V100', 10.243531220759452, 400, 320, 1, 1200000, 4029.3033264614864]\n",
      "['Tesla_V100', 10.427489795415038, 401, 320, 1, 1200000, 3970.9937473684035]\n",
      "['Tesla_V100', 9.980421425550139, 402, 320, 1, 1200000, 4053.539018166463]\n",
      "['Tesla_V100', 9.669934985800388, 403, 320, 1, 1200000, 3992.8613174023744]\n",
      "['Tesla_V100', 9.9301456203559, 404, 320, 1, 1200000, 4034.445268642261]\n",
      "['Tesla_V100', 10.031195122766444, 405, 320, 1, 1200000, 4047.622435672163]\n",
      "['Tesla_V100', 10.1979938197549, 406, 320, 1, 1200000, 4063.3959787815984]\n",
      "['Tesla_V100', 10.042340450742152, 407, 320, 1, 1200000, 4083.6111584359114]\n",
      "['Tesla_V100', 10.022390120984182, 408, 320, 1, 1200000, 4044.269873730693]\n",
      "['Tesla_V100', 10.311258899554824, 409, 320, 1, 1200000, 4072.531899963095]\n",
      "['Tesla_V100', 10.401650435967253, 410, 320, 1, 1200000, 4111.663163742798]\n",
      "['Tesla_V100', 10.263868723940083, 411, 320, 1, 1200000, 4094.5884033366897]\n",
      "['Tesla_V100', 9.656098275536918, 412, 320, 1, 1200000, 4132.480885808481]\n",
      "['Tesla_V100', 9.837081062149931, 413, 320, 1, 1200000, 4124.237450484883]\n",
      "['Tesla_V100', 10.349336335072419, 414, 320, 1, 1200000, 4100.449053316831]\n",
      "['Tesla_V100', 10.294305674659972, 415, 320, 1, 1200000, 4185.216243212733]\n",
      "['Tesla_V100', 10.006250210988146, 416, 320, 1, 1200000, 4130.39569665563]\n",
      "['Tesla_V100', 9.896234481771572, 417, 320, 1, 1200000, 4140.724504131079]\n",
      "['Tesla_V100', 9.62274481505297, 418, 320, 1, 1200000, 4218.7831260675]\n",
      "['Tesla_V100', 9.803421263726229, 419, 320, 1, 1200000, 4170.4477992462425]\n",
      "['Tesla_V100', 10.271091895249487, 420, 320, 1, 1200000, 4202.378879376102]\n",
      "['Tesla_V100', 10.372990572526012, 421, 320, 1, 1200000, 4252.087692321391]\n",
      "['Tesla_V100', 9.580284835347804, 422, 320, 1, 1200000, 4189.875818790893]\n",
      "['Tesla_V100', 10.443103053044526, 423, 320, 1, 1200000, 4205.576111847288]\n",
      "['Tesla_V100', 9.544929781065685, 424, 320, 1, 1200000, 4270.558702919205]\n",
      "['Tesla_V100', 9.573573941728103, 425, 320, 1, 1200000, 4289.256506872776]\n",
      "['Tesla_V100', 10.260279109705513, 426, 320, 1, 1200000, 4251.856781698998]\n",
      "['Tesla_V100', 9.565118480726913, 427, 320, 1, 1200000, 4288.071878008805]\n",
      "['Tesla_V100', 10.069834212933555, 428, 320, 1, 1200000, 4250.158611711353]\n",
      "['Tesla_V100', 10.127341072326704, 429, 320, 1, 1200000, 4303.496426735876]\n",
      "['Tesla_V100', 10.445418950121157, 430, 320, 1, 1200000, 4296.373825479342]\n",
      "['Tesla_V100', 10.012488376059013, 431, 320, 1, 1200000, 4345.216003070853]\n",
      "['Tesla_V100', 9.746402322387434, 432, 320, 1, 1200000, 4299.553139399371]\n",
      "['Tesla_V100', 10.430110771132147, 433, 320, 1, 1200000, 4319.040674255819]\n",
      "['Tesla_V100', 10.2716321304218, 434, 320, 1, 1200000, 4326.0983758713455]\n",
      "['Tesla_V100', 9.71812801275317, 435, 320, 1, 1200000, 4333.36726243829]\n",
      "['Tesla_V100', 10.006965725927788, 436, 320, 1, 1200000, 4335.739307601968]\n",
      "['Tesla_V100', 10.230002504700991, 437, 320, 1, 1200000, 4332.848134075585]\n",
      "['Tesla_V100', 10.053315243390546, 438, 320, 1, 1200000, 4386.715129121773]\n",
      "['Tesla_V100', 9.81567092151335, 439, 320, 1, 1200000, 4350.004276654583]\n",
      "['Tesla_V100', 10.012310059901708, 440, 320, 1, 1200000, 4361.6166322042345]\n",
      "['Tesla_V100', 10.348062973667561, 441, 320, 1, 1200000, 4449.634424870065]\n",
      "['Tesla_V100', 9.764958127868645, 442, 320, 1, 1200000, 4384.761465121647]\n",
      "['Tesla_V100', 9.786725848789766, 443, 320, 1, 1200000, 4403.1228610555]\n",
      "['Tesla_V100', 9.50890399857404, 444, 320, 1, 1200000, 4474.747564284427]\n",
      "['Tesla_V100', 10.289015328653507, 445, 320, 1, 1200000, 4432.456158349906]\n",
      "['Tesla_V100', 10.144826958476258, 446, 320, 1, 1200000, 4501.960598429467]\n",
      "['Tesla_V100', 10.28555503242856, 447, 320, 1, 1200000, 4492.258188497735]\n",
      "['Tesla_V100', 10.107624106230235, 448, 320, 1, 1200000, 4468.346686315501]\n",
      "['Tesla_V100', 10.084639790973611, 449, 320, 1, 1200000, 4459.223871548834]\n",
      "['Tesla_V100', 9.537764097165114, 450, 320, 1, 1200000, 4490.347728525673]\n",
      "['Tesla_V100', 9.654129851719492, 451, 320, 1, 1200000, 4478.677359267083]\n",
      "['Tesla_V100', 9.979830188400241, 452, 320, 1, 1200000, 4490.794967865013]\n",
      "['Tesla_V100', 9.699879080488602, 453, 320, 1, 1200000, 4536.807588288187]\n",
      "['Tesla_V100', 9.957062825844025, 454, 320, 1, 1200000, 4559.805208814548]\n",
      "['Tesla_V100', 9.963050050043828, 455, 320, 1, 1200000, 4532.464897422062]\n",
      "['Tesla_V100', 10.135867785897064, 456, 320, 1, 1200000, 4558.415653332685]\n",
      "['Tesla_V100', 10.477583016264543, 457, 320, 1, 1200000, 4546.945934131288]\n",
      "['Tesla_V100', 10.175057868497579, 458, 320, 1, 1200000, 4624.6431333322835]\n",
      "['Tesla_V100', 10.42599337070773, 459, 320, 1, 1200000, 4567.285655382676]\n",
      "['Tesla_V100', 10.219469122464023, 460, 320, 1, 1200000, 4589.561537733668]\n",
      "['Tesla_V100', 10.259490096521551, 461, 320, 1, 1200000, 4575.490212313454]\n",
      "['Tesla_V100', 10.473095797317953, 462, 320, 1, 1200000, 4587.613664304043]\n",
      "['Tesla_V100', 10.1995359170943, 463, 320, 1, 1200000, 4615.694615555594]\n",
      "['Tesla_V100', 9.686937199695587, 464, 320, 1, 1200000, 4629.871285517797]\n",
      "['Tesla_V100', 10.47135979629769, 465, 320, 1, 1200000, 4614.173364729069]\n",
      "['Tesla_V100', 9.533093704268925, 466, 320, 1, 1200000, 4687.5981145303895]\n",
      "['Tesla_V100', 10.319995708618062, 467, 320, 1, 1200000, 4668.673241458692]\n",
      "['Tesla_V100', 10.460932039063504, 468, 320, 1, 1200000, 4691.954303968931]\n",
      "['Tesla_V100', 10.090997803246779, 469, 320, 1, 1200000, 4721.230092248319]\n",
      "['Tesla_V100', 9.50111618468114, 470, 320, 1, 1200000, 4671.489785513506]\n",
      "['Tesla_V100', 9.787141574249105, 471, 320, 1, 1200000, 4704.982601983887]\n",
      "['Tesla_V100', 9.623795690516086, 472, 320, 1, 1200000, 4753.957997641289]\n",
      "['Tesla_V100', 10.215622206402246, 473, 320, 1, 1200000, 4755.6362867708685]\n",
      "['Tesla_V100', 9.591120433479084, 474, 320, 1, 1200000, 4740.138619794726]\n",
      "['Tesla_V100', 9.619725449163795, 475, 320, 1, 1200000, 4728.533854307857]\n",
      "['Tesla_V100', 10.420547583329304, 476, 320, 1, 1200000, 4785.954158511375]\n",
      "['Tesla_V100', 10.272056821200092, 477, 320, 1, 1200000, 4794.144429287831]\n",
      "['Tesla_V100', 9.904643320330193, 478, 320, 1, 1200000, 4740.512931081162]\n",
      "['Tesla_V100', 10.430877036418403, 479, 320, 1, 1200000, 4808.622934534196]\n",
      "['Tesla_V100', 10.278241727433848, 480, 320, 1, 1200000, 4827.994995630426]\n",
      "['Tesla_V100', 10.323340749279794, 481, 320, 1, 1200000, 4798.681281296944]\n",
      "['Tesla_V100', 10.434781199692678, 482, 320, 1, 1200000, 4817.068296742244]\n",
      "['Tesla_V100', 9.715959560114404, 483, 320, 1, 1200000, 4831.412763793711]\n",
      "['Tesla_V100', 10.143996940094507, 484, 320, 1, 1200000, 4822.519823917585]\n",
      "['Tesla_V100', 9.60680634141745, 485, 320, 1, 1200000, 4845.815904996766]\n",
      "['Tesla_V100', 9.987799270917172, 486, 320, 1, 1200000, 4886.380636332826]\n",
      "['Tesla_V100', 9.572422857200095, 487, 320, 1, 1200000, 4886.599892497631]\n",
      "['Tesla_V100', 9.64798975590526, 488, 320, 1, 1200000, 4897.676808141466]\n",
      "['Tesla_V100', 9.841983197981921, 489, 320, 1, 1200000, 4923.749411186083]\n",
      "['Tesla_V100', 9.956371256168923, 490, 320, 1, 1200000, 4873.903431507817]\n",
      "['Tesla_V100', 10.262524709550723, 491, 320, 1, 1200000, 4957.003870336686]\n",
      "['Tesla_V100', 10.13364325403972, 492, 320, 1, 1200000, 4884.027721227254]\n",
      "['Tesla_V100', 9.704373751882946, 493, 320, 1, 1200000, 4918.134620777523]\n",
      "['Tesla_V100', 9.996577844340015, 494, 320, 1, 1200000, 4892.589956015577]\n",
      "['Tesla_V100', 9.981704172096164, 495, 320, 1, 1200000, 4947.513602887615]\n",
      "['Tesla_V100', 9.935883008454717, 496, 320, 1, 1200000, 4941.018853452114]\n",
      "['Tesla_V100', 10.456941592380284, 497, 320, 1, 1200000, 4986.500920327404]\n",
      "['Tesla_V100', 10.28762840174829, 498, 320, 1, 1200000, 4981.553286677228]\n",
      "['Tesla_V100', 9.866146805268764, 499, 320, 1, 1200000, 4977.147055068833]\n",
      "['Tesla_V100', 9.77338338657984, 500, 320, 1, 1200000, 5004.6963252431715]\n",
      "['Tesla_V100', 10.354137012793286, 501, 320, 1, 1200000, 4963.828070536051]\n",
      "['Tesla_V100', 10.251297977777673, 502, 320, 1, 1200000, 5048.489894679836]\n",
      "['Tesla_V100', 9.89472400969952, 503, 320, 1, 1200000, 5054.525706438501]\n",
      "['Tesla_V100', 9.889287074806685, 504, 320, 1, 1200000, 5021.391030176612]\n",
      "['Tesla_V100', 9.735482475897857, 505, 320, 1, 1200000, 5080.6829600651445]\n",
      "['Tesla_V100', 9.529080849797708, 506, 320, 1, 1200000, 5033.59668872556]\n",
      "['Tesla_V100', 9.801360577333917, 507, 320, 1, 1200000, 5080.9137591104345]\n",
      "['Tesla_V100', 9.925817547294104, 508, 320, 1, 1200000, 5096.567052008324]\n",
      "['Tesla_V100', 9.643061424591053, 509, 320, 1, 1200000, 5074.063597210046]\n",
      "['Tesla_V100', 10.029310212275087, 510, 320, 1, 1200000, 5112.501683133526]\n",
      "['Tesla_V100', 9.769600692783694, 511, 320, 1, 1200000, 5085.59158671008]\n",
      "['Tesla_V100', 9.546383983756659, 512, 320, 1, 1200000, 5153.045554463642]\n",
      "['Tesla_V100', 9.527946300202393, 513, 320, 1, 1200000, 5115.701125066227]\n",
      "['Tesla_V100', 9.679906370530032, 514, 320, 1, 1200000, 5140.181715167662]\n",
      "['Tesla_V100', 10.480648934771505, 515, 320, 1, 1200000, 5141.861489515646]\n",
      "['Tesla_V100', 10.085628134435701, 516, 320, 1, 1200000, 5169.408839993107]\n",
      "['Tesla_V100', 9.556998846314206, 517, 320, 1, 1200000, 5192.865635220291]\n",
      "['Tesla_V100', 10.276421052847047, 518, 320, 1, 1200000, 5204.217811808257]\n",
      "['Tesla_V100', 9.646742838494974, 519, 320, 1, 1200000, 5185.212767207668]\n",
      "['Tesla_V100', 9.897159655315518, 520, 320, 1, 1200000, 5188.119006005548]\n",
      "['Tesla_V100', 9.568292766280965, 521, 320, 1, 1200000, 5240.249751131071]\n",
      "['Tesla_V100', 9.504929126662184, 522, 320, 1, 1200000, 5184.844725794466]\n",
      "['Tesla_V100', 10.19682733113568, 523, 320, 1, 1200000, 5201.489322540575]\n",
      "['Tesla_V100', 10.347017344136027, 524, 320, 1, 1200000, 5250.650369509813]\n",
      "['Tesla_V100', 10.25326065519558, 525, 320, 1, 1200000, 5221.209929313225]\n",
      "['Tesla_V100', 10.251381002285246, 526, 320, 1, 1200000, 5295.352790112291]\n",
      "['Tesla_V100', 10.042017354409133, 527, 320, 1, 1200000, 5220.188037768045]\n",
      "['Tesla_V100', 9.829972887840361, 528, 320, 1, 1200000, 5289.624639818617]\n",
      "['Tesla_V100', 10.438093401810253, 529, 320, 1, 1200000, 5260.111938405326]\n",
      "['Tesla_V100', 9.54371568019489, 530, 320, 1, 1200000, 5310.174484900998]\n",
      "['Tesla_V100', 10.041009013640604, 531, 320, 1, 1200000, 5300.069606602252]\n",
      "['Tesla_V100', 10.453563225813285, 532, 320, 1, 1200000, 5275.768259803304]\n",
      "['Tesla_V100', 10.451102250705011, 533, 320, 1, 1200000, 5280.429863402637]\n",
      "['Tesla_V100', 10.038944404721148, 534, 320, 1, 1200000, 5387.593931974865]\n",
      "['Tesla_V100', 9.961753918229267, 535, 320, 1, 1200000, 5308.296489669203]\n",
      "['Tesla_V100', 10.468688069276162, 536, 320, 1, 1200000, 5354.810372355289]\n",
      "['Tesla_V100', 10.31561097357374, 537, 320, 1, 1200000, 5389.501487964038]\n",
      "['Tesla_V100', 9.90578884062149, 538, 320, 1, 1200000, 5431.907128397176]\n",
      "['Tesla_V100', 10.157141105812185, 539, 320, 1, 1200000, 5417.755356838944]\n",
      "['Tesla_V100', 9.521668930885864, 540, 320, 1, 1200000, 5411.790607115587]\n",
      "['Tesla_V100', 10.445189967451212, 541, 320, 1, 1200000, 5382.085829360781]\n",
      "['Tesla_V100', 9.750297864996941, 542, 320, 1, 1200000, 5457.961157071052]\n",
      "['Tesla_V100', 10.445319910064832, 543, 320, 1, 1200000, 5416.02261202891]\n",
      "['Tesla_V100', 10.175755207212154, 544, 320, 1, 1200000, 5485.22239476632]\n",
      "['Tesla_V100', 9.595123436713173, 545, 320, 1, 1200000, 5448.358679595931]\n",
      "['Tesla_V100', 10.28078852147214, 546, 320, 1, 1200000, 5476.2158607984475]\n",
      "['Tesla_V100', 9.62651208857336, 547, 320, 1, 1200000, 5509.386906694022]\n",
      "['Tesla_V100', 10.118073104703415, 548, 320, 1, 1200000, 5529.392346999869]\n",
      "['Tesla_V100', 10.101802574704987, 549, 320, 1, 1200000, 5524.54144642818]\n",
      "['Tesla_V100', 10.487383335277636, 550, 320, 1, 1200000, 5531.619676947508]\n",
      "['Tesla_V100', 9.991380070256787, 551, 320, 1, 1200000, 5519.749670551779]\n",
      "['Tesla_V100', 10.073246747585028, 552, 320, 1, 1200000, 5470.2600151748275]\n",
      "['Tesla_V100', 9.88529155904522, 553, 320, 1, 1200000, 5521.403742508814]\n",
      "['Tesla_V100', 9.62963159272588, 554, 320, 1, 1200000, 5489.1767432364195]\n",
      "['Tesla_V100', 9.504747504324891, 555, 320, 1, 1200000, 5534.105364014503]\n",
      "['Tesla_V100', 9.742532016252966, 556, 320, 1, 1200000, 5566.158865801177]\n",
      "['Tesla_V100', 10.423934681471183, 557, 320, 1, 1200000, 5518.649864759856]\n",
      "['Tesla_V100', 9.935266159347641, 558, 320, 1, 1200000, 5598.709262765027]\n",
      "['Tesla_V100', 9.584207496789483, 559, 320, 1, 1200000, 5602.518219438349]\n",
      "['Tesla_V100', 9.86909894021736, 560, 320, 1, 1200000, 5629.710280045692]\n",
      "['Tesla_V100', 9.747615084868444, 561, 320, 1, 1200000, 5567.756164284705]\n",
      "['Tesla_V100', 9.607042591424536, 562, 320, 1, 1200000, 5619.411553787295]\n",
      "['Tesla_V100', 10.062065317486788, 563, 320, 1, 1200000, 5598.335000687]\n",
      "['Tesla_V100', 10.050799286735728, 564, 320, 1, 1200000, 5664.573780862363]\n",
      "['Tesla_V100', 9.74980295680609, 565, 320, 1, 1200000, 5594.648638525932]\n",
      "['Tesla_V100', 10.331066581725727, 566, 320, 1, 1200000, 5698.904368920461]\n",
      "['Tesla_V100', 9.877680616884486, 567, 320, 1, 1200000, 5719.6968433708835]\n",
      "['Tesla_V100', 10.32679660092429, 568, 320, 1, 1200000, 5659.253657747831]\n",
      "['Tesla_V100', 10.098192520467265, 569, 320, 1, 1200000, 5679.089032871796]\n",
      "['Tesla_V100', 9.616867148539727, 570, 320, 1, 1200000, 5690.022889668289]\n",
      "['Tesla_V100', 10.343658344859366, 571, 320, 1, 1200000, 5712.639031130253]\n",
      "['Tesla_V100', 10.108401635337259, 572, 320, 1, 1200000, 5703.9695882859605]\n",
      "['Tesla_V100', 9.675803019166361, 573, 320, 1, 1200000, 5738.026474075292]\n",
      "['Tesla_V100', 10.051777773466418, 574, 320, 1, 1200000, 5792.363682896969]\n",
      "['Tesla_V100', 9.50935857551469, 575, 320, 1, 1200000, 5795.787804182917]\n",
      "['Tesla_V100', 10.425847070562314, 576, 320, 1, 1200000, 5730.119866418902]\n",
      "['Tesla_V100', 9.545677082037155, 577, 320, 1, 1200000, 5810.258418972576]\n",
      "['Tesla_V100', 9.668976054412745, 578, 320, 1, 1200000, 5768.079810693773]\n",
      "['Tesla_V100', 9.953198227484478, 579, 320, 1, 1200000, 5769.734876126369]\n",
      "['Tesla_V100', 9.926527532848217, 580, 320, 1, 1200000, 5778.10462858467]\n",
      "['Tesla_V100', 10.02522331098898, 581, 320, 1, 1200000, 5756.549984061351]\n",
      "['Tesla_V100', 10.090969084340498, 582, 320, 1, 1200000, 5779.596047927173]\n",
      "['Tesla_V100', 10.476402600118725, 583, 320, 1, 1200000, 5846.977971764749]\n",
      "['Tesla_V100', 9.507919521912727, 584, 320, 1, 1200000, 5786.163283868082]\n",
      "['Tesla_V100', 9.76973119296703, 585, 320, 1, 1200000, 5880.880557807658]\n",
      "['Tesla_V100', 9.592264347577848, 586, 320, 1, 1200000, 5878.994300850197]\n",
      "['Tesla_V100', 10.051575595078797, 587, 320, 1, 1200000, 5862.548902311579]\n",
      "['Tesla_V100', 9.677337151886396, 588, 320, 1, 1200000, 5825.047802188922]\n",
      "['Tesla_V100', 10.012369831676834, 589, 320, 1, 1200000, 5891.720976680459]\n",
      "['Tesla_V100', 9.514900007504414, 590, 320, 1, 1200000, 5916.8239262617935]\n",
      "['Tesla_V100', 9.861371248098644, 591, 320, 1, 1200000, 5907.790529625851]\n",
      "['Tesla_V100', 9.95908647930259, 592, 320, 1, 1200000, 5943.39470355924]\n",
      "['Tesla_V100', 10.006245205352819, 593, 320, 1, 1200000, 5914.5881717995835]\n",
      "['Tesla_V100', 9.696496845525099, 594, 320, 1, 1200000, 5884.149879383415]\n",
      "['Tesla_V100', 9.726181045802022, 595, 320, 1, 1200000, 5964.495383373598]\n",
      "['Tesla_V100', 9.75814070329542, 596, 320, 1, 1200000, 5981.300401314679]\n",
      "['Tesla_V100', 10.437590618183235, 597, 320, 1, 1200000, 5940.934156429246]\n",
      "['Tesla_V100', 10.273832772835213, 598, 320, 1, 1200000, 5998.676548981945]\n",
      "['Tesla_V100', 9.89958534739453, 599, 320, 1, 1200000, 6014.7180411814315]\n",
      "['Tesla_V100', 9.779673503176875, 600, 320, 1, 1200000, 5975.925474617933]\n",
      "['Tesla_V100', 9.900491978465338, 601, 320, 1, 1200000, 6000.828894902423]\n",
      "['Tesla_V100', 10.0522874003211, 602, 320, 1, 1200000, 6005.959120899992]\n",
      "['Tesla_V100', 10.355483761218212, 603, 320, 1, 1200000, 6046.032421836234]\n",
      "['Tesla_V100', 10.413909070039724, 604, 320, 1, 1200000, 6023.846012574211]\n",
      "['Tesla_V100', 10.484668402125457, 605, 320, 1, 1200000, 6075.56844433569]\n",
      "['Tesla_V100', 10.268243248526744, 606, 320, 1, 1200000, 6069.463367353206]\n",
      "['Tesla_V100', 10.379192559339389, 607, 320, 1, 1200000, 6021.187749598255]\n",
      "['Tesla_V100', 10.279822810292368, 608, 320, 1, 1200000, 6098.49435818529]\n",
      "['Tesla_V100', 10.340991527281874, 609, 320, 1, 1200000, 6136.187453281712]\n",
      "['Tesla_V100', 9.96487839987692, 610, 320, 1, 1200000, 6156.413774851023]\n",
      "['Tesla_V100', 10.313974922513383, 611, 320, 1, 1200000, 6155.102465075023]\n",
      "['Tesla_V100', 9.512877261910939, 612, 320, 1, 1200000, 6174.775758239875]\n",
      "['Tesla_V100', 9.754648615024612, 613, 320, 1, 1200000, 6145.9670343249845]\n",
      "['Tesla_V100', 9.956599130989645, 614, 320, 1, 1200000, 6153.727999938852]\n",
      "['Tesla_V100', 10.081018582103686, 615, 320, 1, 1200000, 6183.368541606368]\n",
      "['Tesla_V100', 10.051548507083277, 616, 320, 1, 1200000, 6167.384272422599]\n",
      "['Tesla_V100', 10.09177411019242, 617, 320, 1, 1200000, 6135.252211318065]\n",
      "['Tesla_V100', 10.183489439671726, 618, 320, 1, 1200000, 6239.706185550979]\n",
      "['Tesla_V100', 10.426118209629966, 619, 320, 1, 1200000, 6157.188529102765]\n",
      "['Tesla_V100', 10.477640970232088, 620, 320, 1, 1200000, 6240.938374305963]\n",
      "['Tesla_V100', 10.466651674268876, 621, 320, 1, 1200000, 6268.958115263685]\n",
      "['Tesla_V100', 9.965731558250353, 622, 320, 1, 1200000, 6235.758425884266]\n",
      "['Tesla_V100', 9.7662897021927, 623, 320, 1, 1200000, 6193.696484187539]\n",
      "['Tesla_V100', 9.948406816661707, 624, 320, 1, 1200000, 6208.877554335077]\n",
      "['Tesla_V100', 10.35260935682012, 625, 320, 1, 1200000, 6194.807984835512]\n",
      "['Tesla_V100', 9.789153373799044, 626, 320, 1, 1200000, 6219.641997509963]\n",
      "['Tesla_V100', 9.598577909926208, 627, 320, 1, 1200000, 6271.228743182353]\n",
      "['Tesla_V100', 10.354799272967092, 628, 320, 1, 1200000, 6271.925509372044]\n",
      "['Tesla_V100', 9.76530283914284, 629, 320, 1, 1200000, 6338.075310842822]\n",
      "['Tesla_V100', 10.421126699777894, 630, 320, 1, 1200000, 6300.730627728616]\n",
      "['Tesla_V100', 10.158125166591862, 631, 320, 1, 1200000, 6254.661129020817]\n",
      "['Tesla_V100', 9.841405301608836, 632, 320, 1, 1200000, 6306.049204525965]\n",
      "['Tesla_V100', 10.005473015440467, 633, 320, 1, 1200000, 6374.138886356723]\n",
      "['Tesla_V100', 10.309566372448769, 634, 320, 1, 1200000, 6277.839789235662]\n",
      "['Tesla_V100', 9.655371186835493, 635, 320, 1, 1200000, 6334.399444415812]\n",
      "['Tesla_V100', 10.012017432529845, 636, 320, 1, 1200000, 6373.60902270043]\n",
      "['Tesla_V100', 9.644397819136698, 637, 320, 1, 1200000, 6395.007453874652]\n",
      "['Tesla_V100', 10.390779205501616, 638, 320, 1, 1200000, 6344.746386994521]\n",
      "['Tesla_V100', 10.039885500705092, 639, 320, 1, 1200000, 6452.591949756616]\n",
      "['Tesla_V100', 10.144128679708588, 640, 320, 1, 1200000, 6347.936511212604]\n",
      "['Tesla_V100', 10.273355017612786, 641, 320, 1, 1200000, 6411.009404032823]\n",
      "['Tesla_V100', 9.612014703054406, 642, 320, 1, 1200000, 6421.945638408483]\n",
      "['Tesla_V100', 10.255239642544662, 643, 320, 1, 1200000, 6380.764446816023]\n",
      "['Tesla_V100', 9.856255967026513, 644, 320, 1, 1200000, 6386.212649459773]\n",
      "['Tesla_V100', 9.651497060132048, 645, 320, 1, 1200000, 6392.272137297856]\n",
      "['Tesla_V100', 9.726143102751413, 646, 320, 1, 1200000, 6454.086814676116]\n",
      "['Tesla_V100', 10.291420936249674, 647, 320, 1, 1200000, 6494.107337420799]\n",
      "['Tesla_V100', 10.087538457439093, 648, 320, 1, 1200000, 6543.06255195453]\n",
      "['Tesla_V100', 9.963694686808342, 649, 320, 1, 1200000, 6543.885795375752]\n",
      "['Tesla_V100', 9.836640130081287, 650, 320, 1, 1200000, 6490.442556798213]\n",
      "['Tesla_V100', 9.897268042299475, 651, 320, 1, 1200000, 6522.605900674973]\n",
      "['Tesla_V100', 10.4320567294969, 652, 320, 1, 1200000, 6554.918198208065]\n",
      "['Tesla_V100', 10.33394560096656, 653, 320, 1, 1200000, 6568.148950841268]\n",
      "['Tesla_V100', 10.45190037865068, 654, 320, 1, 1200000, 6489.538989956124]\n",
      "['Tesla_V100', 10.31439746124389, 655, 320, 1, 1200000, 6572.567740026569]\n",
      "['Tesla_V100', 9.532737889445645, 656, 320, 1, 1200000, 6546.728795513375]\n",
      "['Tesla_V100', 9.818450482594134, 657, 320, 1, 1200000, 6517.859577489689]\n",
      "['Tesla_V100', 9.80985250932519, 658, 320, 1, 1200000, 6528.990667306669]\n",
      "['Tesla_V100', 9.503860026281242, 659, 320, 1, 1200000, 6641.085632212689]\n",
      "['Tesla_V100', 9.656843014901515, 660, 320, 1, 1200000, 6598.3946854003525]\n",
      "['Tesla_V100', 9.555407693372567, 661, 320, 1, 1200000, 6651.233576388797]\n",
      "['Tesla_V100', 10.25607369114698, 662, 320, 1, 1200000, 6662.335828444209]\n",
      "['Tesla_V100', 10.214835106027728, 663, 320, 1, 1200000, 6676.463725492583]\n",
      "['Tesla_V100', 9.79732510829232, 664, 320, 1, 1200000, 6609.805988538453]\n",
      "['Tesla_V100', 9.54215100071891, 665, 320, 1, 1200000, 6712.706844268126]\n",
      "['Tesla_V100', 9.806022807381408, 666, 320, 1, 1200000, 6623.692861957767]\n",
      "['Tesla_V100', 9.568253233193117, 667, 320, 1, 1200000, 6660.638445050625]\n",
      "['Tesla_V100', 10.308608130129699, 668, 320, 1, 1200000, 6655.897570208539]\n",
      "['Tesla_V100', 10.280369999159026, 669, 320, 1, 1200000, 6726.646275408241]\n",
      "['Tesla_V100', 10.436517144411262, 670, 320, 1, 1200000, 6667.412946925865]\n",
      "['Tesla_V100', 10.085045885880366, 671, 320, 1, 1200000, 6767.631166529203]\n",
      "['Tesla_V100', 9.734845632826906, 672, 320, 1, 1200000, 6670.921421228648]\n",
      "['Tesla_V100', 9.880449675461715, 673, 320, 1, 1200000, 6785.241125655892]\n",
      "['Tesla_V100', 10.249536821142808, 674, 320, 1, 1200000, 6677.651990208377]\n",
      "['Tesla_V100', 10.273262060640402, 675, 320, 1, 1200000, 6743.4972325527215]\n",
      "['Tesla_V100', 10.16632039913869, 676, 320, 1, 1200000, 6788.457324567161]\n",
      "['Tesla_V100', 10.052984334403297, 677, 320, 1, 1200000, 6796.630236650561]\n",
      "['Tesla_V100', 10.135313898641538, 678, 320, 1, 1200000, 6804.589117341781]\n",
      "['Tesla_V100', 9.603901456159754, 679, 320, 1, 1200000, 6824.960018098756]\n",
      "['Tesla_V100', 9.990956002086689, 680, 320, 1, 1200000, 6757.674340546598]\n",
      "['Tesla_V100', 9.811548225693228, 681, 320, 1, 1200000, 6826.416672786298]\n",
      "['Tesla_V100', 10.463907924625257, 682, 320, 1, 1200000, 6777.454249985783]\n",
      "['Tesla_V100', 10.39529239844354, 683, 320, 1, 1200000, 6850.049525980693]\n",
      "['Tesla_V100', 9.9974798497577, 684, 320, 1, 1200000, 6822.242709545872]\n",
      "['Tesla_V100', 9.653942798145815, 685, 320, 1, 1200000, 6798.028539943404]\n",
      "['Tesla_V100', 9.630934797273602, 686, 320, 1, 1200000, 6822.699832335665]\n",
      "['Tesla_V100', 10.352633986861202, 687, 320, 1, 1200000, 6834.978944876328]\n",
      "['Tesla_V100', 9.856484887333949, 688, 320, 1, 1200000, 6939.436718910096]\n",
      "['Tesla_V100', 9.980295497878384, 689, 320, 1, 1200000, 6902.689755605044]\n",
      "['Tesla_V100', 10.328734301972148, 690, 320, 1, 1200000, 6850.714135326454]\n",
      "['Tesla_V100', 10.033723121589368, 691, 320, 1, 1200000, 6854.554965687924]\n",
      "['Tesla_V100', 10.042782644165477, 692, 320, 1, 1200000, 6933.062006451333]\n",
      "['Tesla_V100', 10.046089367010904, 693, 320, 1, 1200000, 6911.189842056032]\n",
      "['Tesla_V100', 10.348438514626661, 694, 320, 1, 1200000, 6913.5771797320685]\n",
      "['Tesla_V100', 10.263927545791018, 695, 320, 1, 1200000, 6942.839013870908]\n",
      "['Tesla_V100', 10.305421167250618, 696, 320, 1, 1200000, 7000.582016147379]\n",
      "['Tesla_V100', 10.320899985458478, 697, 320, 1, 1200000, 6996.607385560214]\n",
      "['Tesla_V100', 10.257961809198852, 698, 320, 1, 1200000, 7019.052843789423]\n",
      "['Tesla_V100', 10.251346427655243, 699, 320, 1, 1200000, 6974.91277705819]\n",
      "['Tesla_V100', 9.697441309726145, 700, 320, 1, 1200000, 7065.818322133476]\n",
      "['Tesla_V100', 9.75631014342527, 701, 320, 1, 1200000, 7077.348586107998]\n",
      "['Tesla_V100', 9.602184211448701, 702, 320, 1, 1200000, 7065.907213180938]\n",
      "['Tesla_V100', 9.711517981750832, 703, 320, 1, 1200000, 7076.531309829494]\n",
      "['Tesla_V100', 9.569184804692643, 704, 320, 1, 1200000, 7091.631779461057]\n",
      "['Tesla_V100', 9.918363427608686, 705, 320, 1, 1200000, 7109.90030718784]\n",
      "['Tesla_V100', 10.142279329714095, 706, 320, 1, 1200000, 7062.310476299419]\n",
      "['Tesla_V100', 10.067240555708247, 707, 320, 1, 1200000, 7075.154827208206]\n",
      "['Tesla_V100', 9.63676993677909, 708, 320, 1, 1200000, 7104.294074922265]\n",
      "['Tesla_V100', 9.678717106786767, 709, 320, 1, 1200000, 7137.953285250336]\n",
      "['Tesla_V100', 9.562906961250595, 710, 320, 1, 1200000, 7164.524925040915]\n",
      "['Tesla_V100', 9.648958651134357, 711, 320, 1, 1200000, 7061.008292018409]\n",
      "['Tesla_V100', 9.570088763861836, 712, 320, 1, 1200000, 7070.133727701399]\n",
      "['Tesla_V100', 10.073993703808773, 713, 320, 1, 1200000, 7109.594499225092]\n",
      "['Tesla_V100', 10.433839988135963, 714, 320, 1, 1200000, 7120.9115185869805]\n",
      "['Tesla_V100', 10.426141378321073, 715, 320, 1, 1200000, 7086.875139201547]\n",
      "['Tesla_V100', 9.71993997378114, 716, 320, 1, 1200000, 7209.071491279437]\n",
      "['Tesla_V100', 9.663673074241498, 717, 320, 1, 1200000, 7135.620214013771]\n",
      "['Tesla_V100', 9.871217388114479, 718, 320, 1, 1200000, 7192.090900210294]\n",
      "['Tesla_V100', 9.89231111207304, 719, 320, 1, 1200000, 7156.663291615534]\n",
      "['Tesla_V100', 9.986944176444648, 720, 320, 1, 1200000, 7204.587708526129]\n",
      "['Tesla_V100', 10.462410274608832, 721, 320, 1, 1200000, 7235.372655558022]\n",
      "['Tesla_V100', 9.69217324610704, 722, 320, 1, 1200000, 7257.736091856248]\n",
      "['Tesla_V100', 10.48114176865831, 723, 320, 1, 1200000, 7179.49340979847]\n",
      "['Tesla_V100', 9.662251810271401, 724, 320, 1, 1200000, 7184.205806979697]\n",
      "['Tesla_V100', 9.790761469697477, 725, 320, 1, 1200000, 7278.41626167878]\n",
      "['Tesla_V100', 9.832776846498167, 726, 320, 1, 1200000, 7201.260300578075]\n",
      "['Tesla_V100', 10.129901869753773, 727, 320, 1, 1200000, 7310.005095677104]\n",
      "['Tesla_V100', 9.905041490283237, 728, 320, 1, 1200000, 7304.2885478795]\n",
      "['Tesla_V100', 10.089031122636754, 729, 320, 1, 1200000, 7310.943297182073]\n",
      "['Tesla_V100', 9.92188213601046, 730, 320, 1, 1200000, 7296.656524699543]\n",
      "['Tesla_V100', 10.314949607619994, 731, 320, 1, 1200000, 7322.737237095156]\n",
      "['Tesla_V100', 9.594288103124924, 732, 320, 1, 1200000, 7251.170582345138]\n",
      "['Tesla_V100', 10.011251617982564, 733, 320, 1, 1200000, 7336.547130147293]\n",
      "['Tesla_V100', 10.316168087794315, 734, 320, 1, 1200000, 7373.828125498168]\n",
      "['Tesla_V100', 9.725382918408057, 735, 320, 1, 1200000, 7393.158765194883]\n",
      "['Tesla_V100', 10.129115623882289, 736, 320, 1, 1200000, 7320.737488632881]\n",
      "['Tesla_V100', 10.015873393185428, 737, 320, 1, 1200000, 7331.680076975066]\n",
      "['Tesla_V100', 10.221722870877194, 738, 320, 1, 1200000, 7350.755353823859]\n",
      "['Tesla_V100', 9.996744423608899, 739, 320, 1, 1200000, 7448.853410462825]\n",
      "['Tesla_V100', 10.487361384613992, 740, 320, 1, 1200000, 7467.769000798545]\n",
      "['Tesla_V100', 10.270466396616698, 741, 320, 1, 1200000, 7356.0853238449745]\n",
      "['Tesla_V100', 9.967947099598765, 742, 320, 1, 1200000, 7489.493570687316]\n",
      "['Tesla_V100', 9.853009591949208, 743, 320, 1, 1200000, 7379.4011871204275]\n",
      "['Tesla_V100', 9.691312555731551, 744, 320, 1, 1200000, 7447.343030323051]\n",
      "['Tesla_V100', 9.521701811558234, 745, 320, 1, 1200000, 7490.268954620417]\n",
      "['Tesla_V100', 9.8925690233113, 746, 320, 1, 1200000, 7418.725045674155]\n",
      "['Tesla_V100', 9.79159667942541, 747, 320, 1, 1200000, 7515.289725595545]\n",
      "['Tesla_V100', 9.537181142037193, 748, 320, 1, 1200000, 7520.589599686544]\n",
      "['Tesla_V100', 10.262296912559739, 749, 320, 1, 1200000, 7427.3148746199]\n",
      "['Tesla_V100', 10.331036389076694, 750, 320, 1, 1200000, 7450.806324538779]\n",
      "['Tesla_V100', 9.63643236902047, 751, 320, 1, 1200000, 7459.893459664942]\n",
      "['Tesla_V100', 9.50275800893124, 752, 320, 1, 1200000, 7525.0318860656125]\n",
      "['Tesla_V100', 9.868975851598698, 753, 320, 1, 1200000, 7476.2739365221605]\n",
      "['Tesla_V100', 10.368728576977759, 754, 320, 1, 1200000, 7540.935211294208]\n",
      "['Tesla_V100', 9.848708068876993, 755, 320, 1, 1200000, 7554.940807352993]\n",
      "['Tesla_V100', 10.309211902083543, 756, 320, 1, 1200000, 7510.245342664796]\n",
      "['Tesla_V100', 10.365676609466275, 757, 320, 1, 1200000, 7582.936342088606]\n",
      "['Tesla_V100', 10.115531894530436, 758, 320, 1, 1200000, 7593.422993357168]\n",
      "['Tesla_V100', 9.721319119215087, 759, 320, 1, 1200000, 7542.143310185905]\n",
      "['Tesla_V100', 9.97735231587716, 760, 320, 1, 1200000, 7578.164095680346]\n",
      "['Tesla_V100', 9.740836349004045, 761, 320, 1, 1200000, 7622.369186842986]\n",
      "['Tesla_V100', 10.35618459040959, 762, 320, 1, 1200000, 7628.671214943592]\n",
      "['Tesla_V100', 10.406523229408267, 763, 320, 1, 1200000, 7663.082918233522]\n",
      "['Tesla_V100', 10.400437277633621, 764, 320, 1, 1200000, 7594.421399147601]\n",
      "['Tesla_V100', 10.087336599638487, 765, 320, 1, 1200000, 7627.765695639675]\n",
      "['Tesla_V100', 10.243705106824235, 766, 320, 1, 1200000, 7652.3215651999235]\n",
      "['Tesla_V100', 9.912181156981275, 767, 320, 1, 1200000, 7615.232945253813]\n",
      "['Tesla_V100', 9.85455447468059, 768, 320, 1, 1200000, 7705.774609204335]\n",
      "['Tesla_V100', 10.008772568681758, 769, 320, 1, 1200000, 7692.942488549821]\n",
      "['Tesla_V100', 10.148165872356175, 770, 320, 1, 1200000, 7665.478983018741]\n",
      "['Tesla_V100', 9.720746261385335, 771, 320, 1, 1200000, 7782.1283729904535]\n",
      "['Tesla_V100', 9.637831302216455, 772, 320, 1, 1200000, 7655.306119721552]\n",
      "['Tesla_V100', 10.203650345715584, 773, 320, 1, 1200000, 7752.745574715142]\n",
      "['Tesla_V100', 9.870973251978091, 774, 320, 1, 1200000, 7674.486510829114]\n",
      "['Tesla_V100', 9.779529943378813, 775, 320, 1, 1200000, 7682.144808347511]\n",
      "['Tesla_V100', 10.48912130543775, 776, 320, 1, 1200000, 7790.327221226501]\n",
      "['Tesla_V100', 9.931893100175378, 777, 320, 1, 1200000, 7835.22603782323]\n",
      "['Tesla_V100', 10.065445751711872, 778, 320, 1, 1200000, 7812.874385715942]\n",
      "['Tesla_V100', 10.006769744366967, 779, 320, 1, 1200000, 7840.865264877933]\n",
      "['Tesla_V100', 10.482446493337122, 780, 320, 1, 1200000, 7786.291946276488]\n",
      "['Tesla_V100', 9.647297328144852, 781, 320, 1, 1200000, 7794.668113697845]\n",
      "['Tesla_V100', 10.297711917579948, 782, 320, 1, 1200000, 7741.829202059044]\n",
      "['Tesla_V100', 10.454594782120177, 783, 320, 1, 1200000, 7855.4497667125715]\n",
      "['Tesla_V100', 9.81064783815928, 784, 320, 1, 1200000, 7838.403990882179]\n",
      "['Tesla_V100', 9.7640727436839, 785, 320, 1, 1200000, 7839.941211629836]\n",
      "['Tesla_V100', 10.13821910003163, 786, 320, 1, 1200000, 7887.364949829392]\n",
      "['Tesla_V100', 9.822773343680346, 787, 320, 1, 1200000, 7794.91488618526]\n",
      "['Tesla_V100', 9.877194320509181, 788, 320, 1, 1200000, 7860.359572425051]\n",
      "['Tesla_V100', 9.912024419219781, 789, 320, 1, 1200000, 7963.491479666772]\n",
      "['Tesla_V100', 10.470096975619205, 790, 320, 1, 1200000, 7844.49683299123]\n",
      "['Tesla_V100', 10.413432609962522, 791, 320, 1, 1200000, 7985.692315626676]\n",
      "['Tesla_V100', 9.756647506715927, 792, 320, 1, 1200000, 7958.178185790963]\n",
      "['Tesla_V100', 9.675454849886917, 793, 320, 1, 1200000, 7925.55375030794]\n",
      "['Tesla_V100', 10.107204597958892, 794, 320, 1, 1200000, 7889.118650657675]\n",
      "['Tesla_V100', 10.04718409352151, 795, 320, 1, 1200000, 7987.974175967474]\n",
      "['Tesla_V100', 10.167417760302456, 796, 320, 1, 1200000, 7980.814841907223]\n",
      "['Tesla_V100', 9.62043224220176, 797, 320, 1, 1200000, 8042.748488543071]\n",
      "['Tesla_V100', 9.640917041007185, 798, 320, 1, 1200000, 7997.913186211385]\n",
      "['Tesla_V100', 9.629531401068027, 799, 320, 1, 1200000, 7996.344593611301]\n",
      "['Tesla_V100', 10.36743363003176, 800, 320, 1, 1200000, 8020.845669045664]\n",
      "['Tesla_V100', 10.408586686280541, 801, 320, 1, 1200000, 8035.398791555188]\n",
      "['Tesla_V100', 9.955399699312757, 802, 320, 1, 1200000, 8014.684707738978]\n",
      "['Tesla_V100', 9.711637511220967, 803, 320, 1, 1200000, 8102.890081112748]\n",
      "['Tesla_V100', 9.810925994588972, 804, 320, 1, 1200000, 8023.911297453789]\n",
      "['Tesla_V100', 9.529789543517147, 805, 320, 1, 1200000, 8031.968983828193]\n",
      "['Tesla_V100', 9.61670575692413, 806, 320, 1, 1200000, 8010.257797113594]\n",
      "['Tesla_V100', 10.289538305552474, 807, 320, 1, 1200000, 8065.511476398925]\n",
      "['Tesla_V100', 10.33773907986218, 808, 320, 1, 1200000, 8051.372383130672]\n",
      "['Tesla_V100', 9.883196100349728, 809, 320, 1, 1200000, 8143.739376543372]\n",
      "['Tesla_V100', 9.567631532625857, 810, 320, 1, 1200000, 8151.908437221923]\n",
      "['Tesla_V100', 9.705389957621856, 811, 320, 1, 1200000, 8135.129707984771]\n",
      "['Tesla_V100', 10.032869495661979, 812, 320, 1, 1200000, 8166.979963458199]\n",
      "['Tesla_V100', 10.266497873530327, 813, 320, 1, 1200000, 8142.260691847486]\n",
      "['Tesla_V100', 10.080689298656964, 814, 320, 1, 1200000, 8178.153975271145]\n",
      "['Tesla_V100', 10.110563245443872, 815, 320, 1, 1200000, 8159.615636705533]\n",
      "['Tesla_V100', 9.997695353117345, 816, 320, 1, 1200000, 8234.665600273658]\n",
      "['Tesla_V100', 9.721239946342635, 817, 320, 1, 1200000, 8159.7211997423965]\n",
      "['Tesla_V100', 9.918787394571062, 818, 320, 1, 1200000, 8194.160627912739]\n",
      "['Tesla_V100', 10.183782927335917, 819, 320, 1, 1200000, 8144.589906939118]\n",
      "['Tesla_V100', 10.061857616910885, 820, 320, 1, 1200000, 8268.943762512861]\n",
      "['Tesla_V100', 9.734347093261109, 821, 320, 1, 1200000, 8146.094012057747]\n",
      "['Tesla_V100', 9.7792324145658, 822, 320, 1, 1200000, 8228.67057703819]\n",
      "['Tesla_V100', 9.952481483100852, 823, 320, 1, 1200000, 8220.187490318582]\n",
      "['Tesla_V100', 10.154198564667894, 824, 320, 1, 1200000, 8308.636793713422]\n",
      "['Tesla_V100', 9.68586265267449, 825, 320, 1, 1200000, 8225.014649636269]\n",
      "['Tesla_V100', 9.885165013680952, 826, 320, 1, 1200000, 8202.736781340755]\n",
      "['Tesla_V100', 10.390504754717728, 827, 320, 1, 1200000, 8341.526141162063]\n",
      "['Tesla_V100', 10.499208000188045, 828, 320, 1, 1200000, 8236.481655262915]\n",
      "['Tesla_V100', 9.571751902475661, 829, 320, 1, 1200000, 8231.700338446155]\n",
      "['Tesla_V100', 10.320865649668754, 830, 320, 1, 1200000, 8371.326577929894]\n",
      "['Tesla_V100', 9.85589807665665, 831, 320, 1, 1200000, 8386.866863615836]\n",
      "['Tesla_V100', 10.29695159288903, 832, 320, 1, 1200000, 8295.621188260991]\n",
      "['Tesla_V100', 10.350952915221832, 833, 320, 1, 1200000, 8353.491549594843]\n",
      "['Tesla_V100', 9.946483582798198, 834, 320, 1, 1200000, 8331.981187281881]\n",
      "['Tesla_V100', 10.462119767637907, 835, 320, 1, 1200000, 8315.43481368996]\n",
      "['Tesla_V100', 10.169694831794125, 836, 320, 1, 1200000, 8410.863943058874]\n",
      "['Tesla_V100', 9.885383452382438, 837, 320, 1, 1200000, 8364.0221481871]\n",
      "['Tesla_V100', 10.14227136965768, 838, 320, 1, 1200000, 8394.712133516176]\n",
      "['Tesla_V100', 10.233225871892195, 839, 320, 1, 1200000, 8449.67748629675]\n",
      "['Tesla_V100', 10.041743318164462, 840, 320, 1, 1200000, 8477.182491551712]\n",
      "['Tesla_V100', 9.781135590381961, 841, 320, 1, 1200000, 8337.128493533222]\n",
      "['Tesla_V100', 9.884353732290295, 842, 320, 1, 1200000, 8498.153251571126]\n",
      "['Tesla_V100', 10.263933364519286, 843, 320, 1, 1200000, 8378.895989493409]\n",
      "['Tesla_V100', 10.479705637053057, 844, 320, 1, 1200000, 8474.1680470347]\n",
      "['Tesla_V100', 10.299235926069885, 845, 320, 1, 1200000, 8374.603878607704]\n",
      "['Tesla_V100', 10.222657415317538, 846, 320, 1, 1200000, 8457.229570938314]\n",
      "['Tesla_V100', 10.294926317658378, 847, 320, 1, 1200000, 8506.600176465254]\n",
      "['Tesla_V100', 9.676418761413599, 848, 320, 1, 1200000, 8551.014920218773]\n",
      "['Tesla_V100', 10.077706462851687, 849, 320, 1, 1200000, 8470.596293556187]\n",
      "['Tesla_V100', 9.727706397173396, 850, 320, 1, 1200000, 8548.201663187783]\n",
      "['Tesla_V100', 10.470702390777575, 851, 320, 1, 1200000, 8550.085567593085]\n",
      "['Tesla_V100', 10.109863615190493, 852, 320, 1, 1200000, 8596.737290160572]\n",
      "['Tesla_V100', 10.21157160014888, 853, 320, 1, 1200000, 8522.57423372166]\n",
      "['Tesla_V100', 9.821653841443798, 854, 320, 1, 1200000, 8486.830876554079]\n",
      "['Tesla_V100', 10.373680750238272, 855, 320, 1, 1200000, 8588.114056535904]\n",
      "['Tesla_V100', 10.478078913577713, 856, 320, 1, 1200000, 8487.274818783608]\n",
      "['Tesla_V100', 9.81472405830329, 857, 320, 1, 1200000, 8553.96767970619]\n",
      "['Tesla_V100', 10.462542351591825, 858, 320, 1, 1200000, 8595.578977786676]\n",
      "['Tesla_V100', 10.180083784049021, 859, 320, 1, 1200000, 8550.634154938463]\n",
      "['Tesla_V100', 9.757488306525596, 860, 320, 1, 1200000, 8572.956735198106]\n",
      "['Tesla_V100', 9.583996947278157, 861, 320, 1, 1200000, 8685.684585132052]\n",
      "['Tesla_V100', 9.90516105906421, 862, 320, 1, 1200000, 8568.366478839956]\n",
      "['Tesla_V100', 9.843814035373379, 863, 320, 1, 1200000, 8577.493187904836]\n",
      "['Tesla_V100', 9.755839278336074, 864, 320, 1, 1200000, 8684.512583084344]\n",
      "['Tesla_V100', 9.738022045230839, 865, 320, 1, 1200000, 8684.39991361933]\n",
      "['Tesla_V100', 10.093365519474695, 866, 320, 1, 1200000, 8598.528357259573]\n",
      "['Tesla_V100', 10.263708613147774, 867, 320, 1, 1200000, 8731.188090569613]\n",
      "['Tesla_V100', 10.455987458086323, 868, 320, 1, 1200000, 8759.682287059775]\n",
      "['Tesla_V100', 10.206370797489562, 869, 320, 1, 1200000, 8653.73568087973]\n",
      "['Tesla_V100', 9.537911668278124, 870, 320, 1, 1200000, 8680.900092920496]\n",
      "['Tesla_V100', 10.233126608360598, 871, 320, 1, 1200000, 8786.759246383539]\n",
      "['Tesla_V100', 9.613540362408553, 872, 320, 1, 1200000, 8727.366994376058]\n",
      "['Tesla_V100', 9.974773717836669, 873, 320, 1, 1200000, 8650.684161936439]\n",
      "['Tesla_V100', 10.150677420127789, 874, 320, 1, 1200000, 8805.126930273855]\n",
      "['Tesla_V100', 9.961852606643058, 875, 320, 1, 1200000, 8754.071501450962]\n",
      "['Tesla_V100', 9.677789869058614, 876, 320, 1, 1200000, 8693.61995938926]\n",
      "['Tesla_V100', 10.113847843404244, 877, 320, 1, 1200000, 8807.958335900576]\n",
      "['Tesla_V100', 9.78750337288184, 878, 320, 1, 1200000, 8779.903508946203]\n",
      "['Tesla_V100', 10.458059387930849, 879, 320, 1, 1200000, 8707.958278806864]\n",
      "['Tesla_V100', 9.735813416470133, 880, 320, 1, 1200000, 8730.137372647103]\n",
      "['Tesla_V100', 10.32199243257057, 881, 320, 1, 1200000, 8896.733296699163]\n",
      "['Tesla_V100', 9.659729006634038, 882, 320, 1, 1200000, 8864.435319303435]\n",
      "['Tesla_V100', 9.787378974888014, 883, 320, 1, 1200000, 8799.756305387797]\n",
      "['Tesla_V100', 10.429848094473911, 884, 320, 1, 1200000, 8877.57409065745]\n",
      "['Tesla_V100', 10.312074503587388, 885, 320, 1, 1200000, 8841.992774453627]\n",
      "['Tesla_V100', 9.827262678310174, 886, 320, 1, 1200000, 8795.90795544953]\n",
      "['Tesla_V100', 10.125378179754774, 887, 320, 1, 1200000, 8901.902742066683]\n",
      "['Tesla_V100', 10.430397429092334, 888, 320, 1, 1200000, 8813.427339412458]\n",
      "['Tesla_V100', 10.23706044615136, 889, 320, 1, 1200000, 8898.274814901655]\n",
      "['Tesla_V100', 9.957330106770112, 890, 320, 1, 1200000, 8945.019471628371]\n",
      "['Tesla_V100', 10.10727551523315, 891, 320, 1, 1200000, 8839.210841866636]\n",
      "['Tesla_V100', 9.919156398250621, 892, 320, 1, 1200000, 9008.427562544643]\n",
      "['Tesla_V100', 10.267555224088326, 893, 320, 1, 1200000, 8976.89685554246]\n",
      "['Tesla_V100', 10.422206135640664, 894, 320, 1, 1200000, 8981.014121717573]\n",
      "['Tesla_V100', 10.347984690059542, 895, 320, 1, 1200000, 8932.293931118733]\n",
      "['Tesla_V100', 9.9001446589537, 896, 320, 1, 1200000, 9041.900103116124]\n",
      "['Tesla_V100', 10.257493640135404, 897, 320, 1, 1200000, 8964.853992254753]\n",
      "['Tesla_V100', 9.98959663034315, 898, 320, 1, 1200000, 8893.179699141565]\n",
      "['Tesla_V100', 9.729733588992133, 899, 320, 1, 1200000, 8984.840133419917]\n",
      "['Tesla_V100', 9.793798739452585, 900, 320, 1, 1200000, 9086.726895473512]\n",
      "['Tesla_V100', 9.615522220317082, 901, 320, 1, 1200000, 9011.0015510578]\n",
      "['Tesla_V100', 9.509912599002858, 902, 320, 1, 1200000, 8978.962317062957]\n",
      "['Tesla_V100', 9.724430867457238, 903, 320, 1, 1200000, 9048.9791321299]\n",
      "['Tesla_V100', 10.387218945609302, 904, 320, 1, 1200000, 9052.012984292494]\n",
      "['Tesla_V100', 10.144102544649446, 905, 320, 1, 1200000, 9010.166132163153]\n",
      "['Tesla_V100', 9.808866488796765, 906, 320, 1, 1200000, 9059.819749993476]\n",
      "['Tesla_V100', 10.068559663475497, 907, 320, 1, 1200000, 9078.40406620874]\n",
      "['Tesla_V100', 9.760046346570398, 908, 320, 1, 1200000, 9015.708767707021]\n",
      "['Tesla_V100', 9.86837236397224, 909, 320, 1, 1200000, 9141.652921737195]\n",
      "['Tesla_V100', 10.096133019737687, 910, 320, 1, 1200000, 9042.100893243332]\n",
      "['Tesla_V100', 9.733217739972469, 911, 320, 1, 1200000, 9183.64382383086]\n",
      "['Tesla_V100', 9.896187847395183, 912, 320, 1, 1200000, 9184.515043259033]\n",
      "['Tesla_V100', 10.477471097459519, 913, 320, 1, 1200000, 9104.046568843156]\n",
      "['Tesla_V100', 9.512917682763975, 914, 320, 1, 1200000, 9181.63978360822]\n",
      "['Tesla_V100', 9.985438135334562, 915, 320, 1, 1200000, 9220.898192383112]\n",
      "['Tesla_V100', 9.669229090386901, 916, 320, 1, 1200000, 9112.962951198951]\n",
      "['Tesla_V100', 9.971641735768179, 917, 320, 1, 1200000, 9103.20127988936]\n",
      "['Tesla_V100', 9.860333998408082, 918, 320, 1, 1200000, 9091.0574998606]\n",
      "['Tesla_V100', 10.301780430785309, 919, 320, 1, 1200000, 9116.609410194807]\n",
      "['Tesla_V100', 10.09585794364621, 920, 320, 1, 1200000, 9156.250561596376]\n",
      "['Tesla_V100', 9.530014971255055, 921, 320, 1, 1200000, 9138.105847161476]\n",
      "['Tesla_V100', 10.199215167873968, 922, 320, 1, 1200000, 9292.111890767019]\n",
      "['Tesla_V100', 9.554677749256925, 923, 320, 1, 1200000, 9189.653204521812]\n",
      "['Tesla_V100', 9.898431706073934, 924, 320, 1, 1200000, 9328.460548457824]\n",
      "['Tesla_V100', 9.94266794756831, 925, 320, 1, 1200000, 9310.099262157777]\n",
      "['Tesla_V100', 9.781119876066935, 926, 320, 1, 1200000, 9254.105029834087]\n",
      "['Tesla_V100', 10.32586164031558, 927, 320, 1, 1200000, 9319.815513429254]\n",
      "['Tesla_V100', 9.738566025722667, 928, 320, 1, 1200000, 9234.10436573115]\n",
      "['Tesla_V100', 10.048472985132385, 929, 320, 1, 1200000, 9371.38318789141]\n",
      "['Tesla_V100', 9.619927937975984, 930, 320, 1, 1200000, 9350.603529774033]\n",
      "['Tesla_V100', 10.237600544812212, 931, 320, 1, 1200000, 9375.314425514718]\n",
      "['Tesla_V100', 10.389200367754036, 932, 320, 1, 1200000, 9391.470505130688]\n",
      "['Tesla_V100', 10.400915898603207, 933, 320, 1, 1200000, 9419.699523173893]\n",
      "['Tesla_V100', 9.832429540777145, 934, 320, 1, 1200000, 9420.139316196794]\n",
      "['Tesla_V100', 10.44756031148092, 935, 320, 1, 1200000, 9377.710086085013]\n",
      "['Tesla_V100', 10.164590744071283, 936, 320, 1, 1200000, 9333.52216142976]\n",
      "['Tesla_V100', 10.245303925757442, 937, 320, 1, 1200000, 9389.219921193302]\n",
      "['Tesla_V100', 10.286318977180652, 938, 320, 1, 1200000, 9314.999152439636]\n",
      "['Tesla_V100', 10.31459265848682, 939, 320, 1, 1200000, 9431.951067955792]\n",
      "['Tesla_V100', 10.045945305947923, 940, 320, 1, 1200000, 9354.82670462277]\n",
      "['Tesla_V100', 9.597674204977318, 941, 320, 1, 1200000, 9407.145630241834]\n",
      "['Tesla_V100', 9.644621559074539, 942, 320, 1, 1200000, 9469.318064694455]\n",
      "['Tesla_V100', 10.42455893317533, 943, 320, 1, 1200000, 9490.41259784478]\n",
      "['Tesla_V100', 9.830458909469764, 944, 320, 1, 1200000, 9373.16562102257]\n",
      "['Tesla_V100', 9.507913253289598, 945, 320, 1, 1200000, 9480.931386917167]\n",
      "['Tesla_V100', 9.687612079210801, 946, 320, 1, 1200000, 9500.972078131983]\n",
      "['Tesla_V100', 10.07543281449971, 947, 320, 1, 1200000, 9464.321311270029]\n",
      "['Tesla_V100', 10.267539241619318, 948, 320, 1, 1200000, 9571.210155869492]\n",
      "['Tesla_V100', 9.628594144930972, 949, 320, 1, 1200000, 9499.58844699627]\n",
      "['Tesla_V100', 9.813222885315229, 950, 320, 1, 1200000, 9410.311522243594]\n",
      "['Tesla_V100', 9.529346266919498, 951, 320, 1, 1200000, 9524.763705272977]\n",
      "['Tesla_V100', 9.58291544320366, 952, 320, 1, 1200000, 9433.163941138557]\n",
      "['Tesla_V100', 9.672878533728724, 953, 320, 1, 1200000, 9522.35020803924]\n",
      "['Tesla_V100', 10.000767659479402, 954, 320, 1, 1200000, 9619.232851966304]\n",
      "['Tesla_V100', 9.919669184264167, 955, 320, 1, 1200000, 9475.085753490843]\n",
      "['Tesla_V100', 9.730915924474667, 956, 320, 1, 1200000, 9645.16214080702]\n",
      "['Tesla_V100', 10.095152038187045, 957, 320, 1, 1200000, 9542.312576731687]\n",
      "['Tesla_V100', 10.39271151791968, 958, 320, 1, 1200000, 9541.790017896075]\n",
      "['Tesla_V100', 10.381094700103704, 959, 320, 1, 1200000, 9517.333449917965]\n",
      "['Tesla_V100', 10.438229357229464, 960, 320, 1, 1200000, 9684.035780627724]\n",
      "['Tesla_V100', 9.845269849168801, 961, 320, 1, 1200000, 9517.272871711972]\n",
      "['Tesla_V100', 9.853041434225224, 962, 320, 1, 1200000, 9558.588039331156]\n",
      "['Tesla_V100', 9.766067101443733, 963, 320, 1, 1200000, 9654.206522162038]\n",
      "['Tesla_V100', 9.94309818564492, 964, 320, 1, 1200000, 9685.561087040614]\n",
      "['Tesla_V100', 10.137037038468764, 965, 320, 1, 1200000, 9611.507540667262]\n",
      "['Tesla_V100', 9.823876538532062, 966, 320, 1, 1200000, 9717.60188576506]\n",
      "['Tesla_V100', 10.032163048222355, 967, 320, 1, 1200000, 9597.93540664741]\n",
      "['Tesla_V100', 10.08672311703197, 968, 320, 1, 1200000, 9609.91272254028]\n",
      "['Tesla_V100', 10.368708165111983, 969, 320, 1, 1200000, 9618.152563777701]\n",
      "['Tesla_V100', 10.169408119891372, 970, 320, 1, 1200000, 9776.981212102857]\n",
      "['Tesla_V100', 9.598123525450921, 971, 320, 1, 1200000, 9715.871089339513]\n",
      "['Tesla_V100', 10.458090873685164, 972, 320, 1, 1200000, 9676.832076201044]\n",
      "['Tesla_V100', 9.781067939947699, 973, 320, 1, 1200000, 9724.172197071757]\n",
      "['Tesla_V100', 10.44770329372528, 974, 320, 1, 1200000, 9798.637467232465]\n",
      "['Tesla_V100', 9.771185632509287, 975, 320, 1, 1200000, 9697.659144488081]\n",
      "['Tesla_V100', 9.698616750084447, 976, 320, 1, 1200000, 9746.282417525405]\n",
      "['Tesla_V100', 10.307905678104921, 977, 320, 1, 1200000, 9702.059851046271]\n",
      "['Tesla_V100', 10.117059679263846, 978, 320, 1, 1200000, 9774.579999558238]\n",
      "['Tesla_V100', 10.146320809887229, 979, 320, 1, 1200000, 9771.102401803128]\n",
      "['Tesla_V100', 10.259822211188089, 980, 320, 1, 1200000, 9764.917475362923]\n",
      "['Tesla_V100', 10.222167393295928, 981, 320, 1, 1200000, 9900.184974222266]\n",
      "['Tesla_V100', 10.07924175138723, 982, 320, 1, 1200000, 9846.427715170712]\n",
      "['Tesla_V100', 10.02480119975858, 983, 320, 1, 1200000, 9827.237844113299]\n",
      "['Tesla_V100', 9.668546819523467, 984, 320, 1, 1200000, 9876.909634848485]\n",
      "['Tesla_V100', 10.106171863812486, 985, 320, 1, 1200000, 9860.418494814921]\n",
      "['Tesla_V100', 9.80642697869684, 986, 320, 1, 1200000, 9839.193705671423]\n",
      "['Tesla_V100', 9.933193855845465, 987, 320, 1, 1200000, 9901.087558008872]\n",
      "['Tesla_V100', 10.225893626758559, 988, 320, 1, 1200000, 9843.039603124962]\n",
      "['Tesla_V100', 10.114736496536624, 989, 320, 1, 1200000, 9864.3600303472]\n",
      "['Tesla_V100', 9.985211673053005, 990, 320, 1, 1200000, 9817.219665034947]\n",
      "['Tesla_V100', 10.078677624362179, 991, 320, 1, 1200000, 9988.57482803104]\n",
      "['Tesla_V100', 10.453405346503349, 992, 320, 1, 1200000, 9887.191610024196]\n",
      "['Tesla_V100', 9.670126100631085, 993, 320, 1, 1200000, 10024.660822351974]\n",
      "['Tesla_V100', 10.29670500194622, 994, 320, 1, 1200000, 9898.883478693864]\n",
      "['Tesla_V100', 9.504490043973007, 995, 320, 1, 1200000, 9903.07664858061]\n",
      "['Tesla_V100', 10.454945266914685, 996, 320, 1, 1200000, 9954.873945163825]\n",
      "['Tesla_V100', 9.794314013877653, 997, 320, 1, 1200000, 9938.573107541828]\n",
      "['Tesla_V100', 9.729196834969418, 998, 320, 1, 1200000, 9993.034759399772]\n",
      "['Tesla_V100', 9.717167064132243, 999, 320, 1, 1200000, 10029.139132232354]\n"
     ]
    }
   ],
   "source": [
    "def dummy_sim(gpu, flops, imgnum, img_size, batch_size, model_params, epoch_time, n) :\n",
    "    GPU = gpu\n",
    "    flops = np.random.uniform(flops-(flops*0.05),flops+(flops*0.05))\n",
    "    img_num = imgnum*n\n",
    "    img_size = img_size\n",
    "    batch_size = batch_size\n",
    "    model_param = model_params\n",
    "    epoch_time = epoch_time*n + (epoch_time*n)*np.random.uniform(-0.01, 0.01)\n",
    "    return [GPU, flops, img_num, img_size, batch_size, model_param, epoch_time]\n",
    "\n",
    "\n",
    "\n",
    "dummy_data = {\n",
    "    \"GPU\": [],\n",
    "    \"flops\": [],  # Tesla V100의 대략적인 Flops 범위\n",
    "    \"img_num\": [],\n",
    "    \"img_size\": [],\n",
    "    \"batch_size\": [],\n",
    "    \"model_param_num\": [],\n",
    "    \"epoch_time\": []\n",
    "}\n",
    "\n",
    "gpu = \"Tesla_V100\" \n",
    "flops_baund = 10\n",
    "base_img_num = 1\n",
    "base_img_size = 320\n",
    "base_batch_size = 1\n",
    "base_model_param = 1200000\n",
    "base_epoch_time = 10\n",
    "\n",
    "for n in range(1000) : \n",
    "    dummy = dummy_sim(gpu, flops_baund, base_img_num, base_img_size, base_batch_size, base_model_param, base_epoch_time, n)\n",
    "    print(dummy)\n",
    "    #add dummy\n",
    "    dummy_data[\"GPU\"].append(dummy[0])\n",
    "    dummy_data[\"flops\"].append(dummy[1])\n",
    "    dummy_data[\"img_num\"].append(dummy[2])\n",
    "    dummy_data[\"img_size\"].append(dummy[3])\n",
    "    dummy_data[\"batch_size\"].append(dummy[4])\n",
    "    dummy_data[\"model_param_num\"].append(dummy[5])\n",
    "    dummy_data[\"epoch_time\"].append(dummy[6])\n",
    "\n",
    "df = pd.DataFrame(dummy_data)\n",
    "\n",
    "csv_file_path = f\"/DATA/datasets/job_time/training_time_dummy_dataset_b{base_batch_size}.csv\"\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/DATA/datasets/job_time/training_time_dummy_dataset_b1.csv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPU</th>\n",
       "      <th>flops</th>\n",
       "      <th>img_num</th>\n",
       "      <th>img_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>model_param_num</th>\n",
       "      <th>epoch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.746535</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>10.406354</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>10.016021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>10.097937</td>\n",
       "      <td>2</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>19.895217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>10.080887</td>\n",
       "      <td>3</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>29.787522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.835631</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>40.100422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.504490</td>\n",
       "      <td>995</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>9903.076649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>10.454945</td>\n",
       "      <td>996</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>9954.873945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.794314</td>\n",
       "      <td>997</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>9938.573108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.729197</td>\n",
       "      <td>998</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>9993.034759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Tesla_V100</td>\n",
       "      <td>9.717167</td>\n",
       "      <td>999</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1200000</td>\n",
       "      <td>10029.139132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            GPU      flops  img_num  img_size  batch_size  model_param_num  \\\n",
       "0    Tesla_V100   9.746535        0       320           1          1200000   \n",
       "1    Tesla_V100  10.406354        1       320           1          1200000   \n",
       "2    Tesla_V100  10.097937        2       320           1          1200000   \n",
       "3    Tesla_V100  10.080887        3       320           1          1200000   \n",
       "4    Tesla_V100   9.835631        4       320           1          1200000   \n",
       "..          ...        ...      ...       ...         ...              ...   \n",
       "995  Tesla_V100   9.504490      995       320           1          1200000   \n",
       "996  Tesla_V100  10.454945      996       320           1          1200000   \n",
       "997  Tesla_V100   9.794314      997       320           1          1200000   \n",
       "998  Tesla_V100   9.729197      998       320           1          1200000   \n",
       "999  Tesla_V100   9.717167      999       320           1          1200000   \n",
       "\n",
       "       epoch_time  \n",
       "0        0.000000  \n",
       "1       10.016021  \n",
       "2       19.895217  \n",
       "3       29.787522  \n",
       "4       40.100422  \n",
       "..            ...  \n",
       "995   9903.076649  \n",
       "996   9954.873945  \n",
       "997   9938.573108  \n",
       "998   9993.034759  \n",
       "999  10029.139132  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1424794/2064779351.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"t_instance_num\", axis=1, inplace=True)\n",
      "/tmp/ipykernel_1424794/2064779351.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"v_instance_num\", axis=1, inplace=True)\n",
      "/tmp/ipykernel_1424794/2064779351.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"elapsed\", axis=1, inplace=True)\n",
      "/tmp/ipykernel_1424794/2064779351.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"remaining\", axis=1, inplace=True)\n",
      "/tmp/ipykernel_1424794/2064779351.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"gpu_usage\", axis=1, inplace=True)\n",
      "/tmp/ipykernel_1424794/2064779351.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2.drop(\"cpu_usage\", axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"/DATA/yolov8l.csv\")\n",
    "df = df_all\n",
    "# one hot\n",
    "gpu_dummies = pd.get_dummies(df['gpu'])\n",
    "df_co = pd.concat([df, gpu_dummies], axis=1)\n",
    "#df.drop('GPU', axis=1, inplace=True)\n",
    "df_2 = df_co[df_co['epoch'] != 0] # 0 epoch 제거\n",
    "\n",
    "df_2.drop(\"t_instance_num\", axis=1, inplace=True)\n",
    "df_2.drop(\"v_instance_num\", axis=1, inplace=True)\n",
    "df_2.drop(\"elapsed\", axis=1, inplace=True)\n",
    "df_2.drop(\"remaining\", axis=1, inplace=True)\n",
    "df_2.drop(\"gpu_usage\", axis=1, inplace=True)\n",
    "df_2.drop(\"cpu_usage\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1_rows = df_2[df_2['batch']==1]\n",
    "batch2_rows = df_2[df_2['batch']==2]\n",
    "batch4_rows = df_2[df_2['batch']==4]\n",
    "batch8_rows = df_2[df_2['batch']==8]\n",
    "batch16_rows = df_2[df_2['batch']==16]\n",
    "# batch32_rows = df_2[df_2['batch']==32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1_time = batch1_rows['time'].tolist()\n",
    "batch2_time = batch2_rows['time'].tolist()\n",
    "batch4_time = batch4_rows['time'].tolist()\n",
    "batch8_time = batch8_rows['time'].tolist()\n",
    "batch16_time = batch16_rows['time'].tolist()\n",
    "# batch32_time = batch32_rows['time'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeInfo(time_list) :\n",
    "    mean = np.mean(time_list)\n",
    "    min = np.min(time_list)\n",
    "    max = np.max(time_list)\n",
    "    min_calc = abs(mean - min)\n",
    "    max_calc = abs(mean - max)\n",
    "    print(f\"mean: {mean}, min: {min}, max: {max}, min_calc: {min_calc}, max_clac: {max_calc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 115.99973954699999, min: 115.5111253, max: 120.1118021, min_calc: 0.4886142469999868, max_clac: 4.112062553000015\n",
      "mean: 59.86130603820001, min: 59.30251053, max: 62.4683938, min_calc: 0.5587955082000136, max_clac: 2.6070877617999884\n",
      "mean: 30.739883732000006, min: 30.14181229, max: 34.65681243, min_calc: 0.5980714420000055, max_clac: 3.916928697999996\n",
      "mean: 17.3051782593, min: 16.95409637, max: 19.89364481, min_calc: 0.3510818893000014, max_clac: 2.5884665507000015\n",
      "mean: 11.095385533800002, min: 10.52288845, max: 13.08043528, min_calc: 0.5724970838000019, max_clac: 1.9850497461999979\n"
     ]
    }
   ],
   "source": [
    "data_list = [batch1_time, batch2_time, batch4_time, batch8_time, batch16_time]\n",
    "for data in data_list :\n",
    "    TimeInfo(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03087507341219901 0.04488631596610454\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dummy_sim(gpu, flops, imgnum, img_size, batch_size, model_params, epoch_time, n, rate_min, rate_max) :\n",
    "    GPU = gpu\n",
    "    flops = flops\n",
    "    img_num = int(imgnum*n)\n",
    "    img_size = img_size\n",
    "    batch_size = batch_size\n",
    "    model_param = model_params\n",
    "    epoch_time = epoch_time*n #+ (epoch_time*n)*np.random.uniform(rate_min, rate_max)\n",
    "    #epoch_time = epoch_time*n + (epoch_time*n)*np.random.uniform(0.005, 0.005)\n",
    "    return [GPU, flops, img_num, img_size, batch_size, model_param, epoch_time]\n",
    "\n",
    "\n",
    "\n",
    "dummy_data = {\n",
    "    \"GPU\": [],\n",
    "    \"flops\": [],  # Tesla V100의 대략적인 Flops 범위\n",
    "    \"t_img_num\": [],\n",
    "    \"v_img_num\": [],\n",
    "    \"img_size\": [],\n",
    "    \"batch_size\": [],\n",
    "    \"model_param_num\": [],\n",
    "    \"epoch_time\": []\n",
    "}\n",
    "'''\n",
    "mean: 119.19498045700001, min: 115.5111253, max: 124.5536487, min_calc: 3.6838551570000106, max_clac: 5.358668242999983\n",
    "mean: 62.00161856233334, min: 59.30251053, max: 66.71125674, min_calc: 2.699108032333342, max_clac: 4.709638177666655\n",
    "mean: 32.664159439033334, min: 30.14181229, max: 37.35439503, min_calc: 2.522347149033333, max_clac: 4.690235590966665\n",
    "mean: 21.59520148616667, min: 16.95409637, max: 32.45922494, min_calc: 4.6411051161666705, max_clac: 10.86402345383333\n",
    "mean: 14.797176456099999, min: 10.52288845, max: 30.14607501, min_calc: 4.274288006099999, max_clac: 15.348898553900002\n",
    "mean: 8.8840757064, min: 7.580267978, max: 27.80710959, min_calc: 1.3038077284000007, max_clac: 18.9230338836\n",
    "'''\n",
    "\n",
    "gpu = \"Tesla_V100\" \n",
    "flops_baund = 14\n",
    "base_img_num = 100\n",
    "base_img_size = 640\n",
    "base_model_param = 43643718\n",
    "\n",
    "# base_batch_size = 1\n",
    "# base_epoch_time = 119.19 #batch1\n",
    "# rate_min = 3.68 #batch1\n",
    "# rate_max = 5.35 #batch1\n",
    "\n",
    "# base_batch_size = 2\n",
    "# base_epoch_time = 62 #batch2\n",
    "# rate_min = 2.69 #batch2\n",
    "# rate_max = 4.70 #batch2\n",
    "\n",
    "# base_batch_size = 4\n",
    "# base_epoch_time = 32.66 #batch4\n",
    "# rate_min = 2.52 #batch4\n",
    "# rate_max = 4.69 #batch4\n",
    "\n",
    "# base_batch_size = 8\n",
    "# base_epoch_time = 21.59 #batch8\n",
    "# rate_min = 4.64 #batch8\n",
    "# rate_max = 10.86 #batch8\n",
    "\n",
    "# base_batch_size = 16\n",
    "# base_epoch_time = 14.79 #batch16\n",
    "# rate_min = 4.27 #batch16\n",
    "# rate_max = 15.34 #batch16\n",
    "\n",
    "# base_batch_size = 32\n",
    "# base_epoch_time = 8.88 #batch32\n",
    "# rate_min = 1.36 #batch32\n",
    "# rate_max = 18.92 #batch32\n",
    "\n",
    "data1_time = base_epoch_time/base_img_num\n",
    "data1 = base_img_num/base_img_num\n",
    "rate_min = rate_min/base_epoch_time\n",
    "rate_max = rate_max/base_epoch_time\n",
    "print(rate_min, rate_max)\n",
    "\n",
    "for n in range(2000) : \n",
    "    dummy = dummy_sim(gpu, flops_baund, data1, base_img_size, base_batch_size, \n",
    "                      base_model_param, data1_time, n, rate_min, rate_max)\n",
    "    #print(dummy)\n",
    "    #add dummy\n",
    "    dummy_data[\"GPU\"].append(dummy[0])\n",
    "    dummy_data[\"flops\"].append(dummy[1])\n",
    "    dummy_data[\"t_img_num\"].append(dummy[2])\n",
    "    dummy_data[\"v_img_num\"].append(dummy[2])\n",
    "    dummy_data[\"img_size\"].append(dummy[3])\n",
    "    dummy_data[\"batch_size\"].append(dummy[4])\n",
    "    dummy_data[\"model_param_num\"].append(dummy[5])\n",
    "    dummy_data[\"epoch_time\"].append(dummy[6])\n",
    "\n",
    "df = pd.DataFrame(dummy_data)\n",
    "\n",
    "csv_file_path = f\"/DATA/datasets/job_time/training_time_dummy_dataset_b{base_batch_size}.csv\"\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flops</th>\n",
       "      <th>t_img_num</th>\n",
       "      <th>v_img_num</th>\n",
       "      <th>img_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>model_param_num</th>\n",
       "      <th>epoch_time</th>\n",
       "      <th>Tesla_V100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>43643718</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>43643718</td>\n",
       "      <td>1.1919</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>43643718</td>\n",
       "      <td>2.3838</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>43643718</td>\n",
       "      <td>3.5757</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "      <td>43643718</td>\n",
       "      <td>4.7676</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>14</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>640</td>\n",
       "      <td>32</td>\n",
       "      <td>43643718</td>\n",
       "      <td>177.1560</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>640</td>\n",
       "      <td>32</td>\n",
       "      <td>43643718</td>\n",
       "      <td>177.2448</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>14</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>640</td>\n",
       "      <td>32</td>\n",
       "      <td>43643718</td>\n",
       "      <td>177.3336</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>14</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>640</td>\n",
       "      <td>32</td>\n",
       "      <td>43643718</td>\n",
       "      <td>177.4224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>14</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>640</td>\n",
       "      <td>32</td>\n",
       "      <td>43643718</td>\n",
       "      <td>177.5112</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       flops  t_img_num  v_img_num  img_size  batch_size  model_param_num  \\\n",
       "0         14          0          0       640           1         43643718   \n",
       "1         14          1          1       640           1         43643718   \n",
       "2         14          2          2       640           1         43643718   \n",
       "3         14          3          3       640           1         43643718   \n",
       "4         14          4          4       640           1         43643718   \n",
       "...      ...        ...        ...       ...         ...              ...   \n",
       "11995     14       1995       1995       640          32         43643718   \n",
       "11996     14       1996       1996       640          32         43643718   \n",
       "11997     14       1997       1997       640          32         43643718   \n",
       "11998     14       1998       1998       640          32         43643718   \n",
       "11999     14       1999       1999       640          32         43643718   \n",
       "\n",
       "       epoch_time  Tesla_V100  \n",
       "0          0.0000        True  \n",
       "1          1.1919        True  \n",
       "2          2.3838        True  \n",
       "3          3.5757        True  \n",
       "4          4.7676        True  \n",
       "...           ...         ...  \n",
       "11995    177.1560        True  \n",
       "11996    177.2448        True  \n",
       "11997    177.3336        True  \n",
       "11998    177.4224        True  \n",
       "11999    177.5112        True  \n",
       "\n",
       "[12000 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot\n",
    "df1 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b1.csv\")\n",
    "df2 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b2.csv\")\n",
    "df3 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b4.csv\")\n",
    "df4 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b8.csv\")\n",
    "df5 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b16.csv\")\n",
    "df6 = pd.read_csv(\"/DATA/datasets/job_time/training_time_dummy_dataset_b32.csv\")\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "\n",
    "gpu_dummies = pd.get_dummies(df['GPU'])\n",
    "df = pd.concat([df, gpu_dummies], axis=1)\n",
    "df.drop('GPU', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "X = df.drop('epoch_time', axis=1).values\n",
    "#X = df.drop('Instance Num', axis=1).values\n",
    "y = df['epoch_time'].values\n",
    "\n",
    "# data slice\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5, random_state=42)\n",
    "\n",
    "# fit\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "#torch tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# data loader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_data, batch_size = 2, shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8400, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 206068.4643, Validation Loss: 80798110.9623\n",
      "Epoch [2/200], Train Loss: 194923.2848, Validation Loss: 2555873958.2907\n",
      "Epoch [3/200], Train Loss: 204700.6514, Validation Loss: 592700547.3425\n",
      "Epoch [4/200], Train Loss: 194615.2077, Validation Loss: 1758742974.7420\n",
      "Epoch [5/200], Train Loss: 191826.8119, Validation Loss: 3154908008.6437\n",
      "Epoch [6/200], Train Loss: 195378.8204, Validation Loss: 738097.4031\n",
      "Epoch [7/200], Train Loss: 194303.9041, Validation Loss: 6706310.6848\n",
      "Epoch [8/200], Train Loss: 193767.3357, Validation Loss: 168640331.5596\n",
      "Epoch [9/200], Train Loss: 192674.6263, Validation Loss: 691622455.5895\n",
      "Epoch [10/200], Train Loss: 193752.6900, Validation Loss: 509835542.3747\n",
      "Epoch [11/200], Train Loss: 190994.3658, Validation Loss: 17824696.2269\n",
      "Epoch [12/200], Train Loss: 184772.5062, Validation Loss: 8501516391.2449\n",
      "Epoch [13/200], Train Loss: 182471.7985, Validation Loss: 9445389592.8986\n",
      "Epoch [14/200], Train Loss: 185671.6383, Validation Loss: 19548751842.8613\n",
      "Epoch [15/200], Train Loss: 185612.4260, Validation Loss: 31640493615.5134\n",
      "Epoch [16/200], Train Loss: 182822.9735, Validation Loss: 24484811678.9337\n",
      "Epoch [17/200], Train Loss: 199429.3008, Validation Loss: 21959102603.2580\n",
      "Epoch [18/200], Train Loss: 185447.9513, Validation Loss: 4908822229.0476\n",
      "Epoch [19/200], Train Loss: 179870.7536, Validation Loss: 55693346911.0240\n",
      "Epoch [20/200], Train Loss: 179577.0840, Validation Loss: 35678743187.8777\n",
      "Epoch [21/200], Train Loss: 175416.1384, Validation Loss: 47983606453.4799\n",
      "Epoch [22/200], Train Loss: 171010.6751, Validation Loss: 17498836278.9612\n",
      "Epoch [23/200], Train Loss: 168565.0102, Validation Loss: 18643835276.4597\n",
      "Epoch [24/200], Train Loss: 166836.0572, Validation Loss: 17152193075.1188\n",
      "Epoch [25/200], Train Loss: 164746.2451, Validation Loss: 63486593282.6055\n",
      "Epoch [26/200], Train Loss: 159909.7996, Validation Loss: 63830005987.8073\n",
      "Epoch [27/200], Train Loss: 158854.8284, Validation Loss: 53971983371.5127\n",
      "Epoch [28/200], Train Loss: 157606.9211, Validation Loss: 21963518909.6418\n",
      "Epoch [29/200], Train Loss: 154625.4862, Validation Loss: 73626056440.7346\n",
      "Epoch [30/200], Train Loss: 152770.9319, Validation Loss: 54255585930.8254\n",
      "Epoch [31/200], Train Loss: 154033.5387, Validation Loss: 12746472094.0844\n",
      "Epoch [32/200], Train Loss: 148085.8911, Validation Loss: 4585527805.7844\n",
      "Epoch [33/200], Train Loss: 152881.5312, Validation Loss: 65698440577.8846\n",
      "Epoch [34/200], Train Loss: 153414.1447, Validation Loss: 37173878569.0216\n",
      "Epoch [35/200], Train Loss: 151133.1525, Validation Loss: 31344441154.7982\n",
      "Epoch [36/200], Train Loss: 154930.7929, Validation Loss: 29558623489.2998\n",
      "Epoch [37/200], Train Loss: 152098.7961, Validation Loss: 4941288322.9021\n",
      "Epoch [38/200], Train Loss: 150587.4887, Validation Loss: 17778441881.7659\n",
      "Epoch [39/200], Train Loss: 148952.2962, Validation Loss: 42029670750.2348\n",
      "Epoch [40/200], Train Loss: 153460.7505, Validation Loss: 37595910786.4584\n",
      "Epoch [41/200], Train Loss: 151962.0766, Validation Loss: 54979798721.0296\n",
      "Epoch [42/200], Train Loss: 152257.2809, Validation Loss: 48784166626.1475\n",
      "Epoch [43/200], Train Loss: 148285.5927, Validation Loss: 13054361946.4641\n",
      "Epoch [44/200], Train Loss: 151236.4292, Validation Loss: 37294977523.8034\n",
      "Epoch [45/200], Train Loss: 151886.8713, Validation Loss: 5472325436.7098\n",
      "Epoch [46/200], Train Loss: 150530.6562, Validation Loss: 34096218238.8151\n",
      "Epoch [47/200], Train Loss: 151585.8135, Validation Loss: 12727081880.5338\n",
      "Epoch [48/200], Train Loss: 146431.4354, Validation Loss: 2544658737.9444\n",
      "Epoch [49/200], Train Loss: 152479.1278, Validation Loss: 35594024578.1987\n",
      "Epoch [50/200], Train Loss: 150804.0939, Validation Loss: 6715537958.9172\n",
      "Epoch [51/200], Train Loss: 147656.0901, Validation Loss: 16288304494.6283\n",
      "Epoch [52/200], Train Loss: 150164.9113, Validation Loss: 12110379522.2148\n",
      "Epoch [53/200], Train Loss: 151057.4493, Validation Loss: 22310465605.9617\n",
      "Epoch [54/200], Train Loss: 150293.1022, Validation Loss: 25471526067.4846\n",
      "Epoch [55/200], Train Loss: 149323.6016, Validation Loss: 41861668100.7124\n",
      "Epoch [56/200], Train Loss: 148051.0525, Validation Loss: 5895047792.8129\n",
      "Epoch [57/200], Train Loss: 152633.6332, Validation Loss: 39366293222.9904\n",
      "Epoch [58/200], Train Loss: 149698.9722, Validation Loss: 47238536779.9394\n",
      "Epoch [59/200], Train Loss: 150988.0080, Validation Loss: 12621936265.6788\n",
      "Epoch [60/200], Train Loss: 151075.6597, Validation Loss: 18132183801.7243\n",
      "Epoch [61/200], Train Loss: 149160.6751, Validation Loss: 39201019684.9926\n",
      "Epoch [62/200], Train Loss: 149800.9204, Validation Loss: 15002389390.1386\n",
      "Epoch [63/200], Train Loss: 150492.9202, Validation Loss: 17605036161.3564\n",
      "Epoch [64/200], Train Loss: 151251.3843, Validation Loss: 32773210535.1229\n",
      "Epoch [65/200], Train Loss: 151633.9880, Validation Loss: 54901696166.4758\n",
      "Epoch [66/200], Train Loss: 146765.0636, Validation Loss: 22527415532.1070\n",
      "Epoch [67/200], Train Loss: 151063.2749, Validation Loss: 62580255502.6346\n",
      "Epoch [68/200], Train Loss: 155894.8631, Validation Loss: 23475914605.6509\n",
      "Epoch [69/200], Train Loss: 150648.0893, Validation Loss: 34801092601.8824\n",
      "Epoch [70/200], Train Loss: 147498.4602, Validation Loss: 8960371105.9909\n",
      "Epoch [71/200], Train Loss: 146186.9807, Validation Loss: 33075929552.1950\n",
      "Epoch [72/200], Train Loss: 150287.8726, Validation Loss: 53533634052.5255\n",
      "Epoch [73/200], Train Loss: 149769.3036, Validation Loss: 49583438369.6473\n",
      "Epoch [74/200], Train Loss: 152322.5627, Validation Loss: 17147220201.9896\n",
      "Epoch [75/200], Train Loss: 147553.2657, Validation Loss: 30749320331.5548\n",
      "Epoch [76/200], Train Loss: 148300.6020, Validation Loss: 25978998287.6686\n",
      "Epoch [77/200], Train Loss: 148962.7649, Validation Loss: 37858878031.7300\n",
      "Epoch [78/200], Train Loss: 148933.0520, Validation Loss: 46433691859.3916\n",
      "Epoch [79/200], Train Loss: 149851.5066, Validation Loss: 14712135221.8977\n",
      "Epoch [80/200], Train Loss: 150853.2646, Validation Loss: 34906846429.8920\n",
      "Epoch [81/200], Train Loss: 147062.6726, Validation Loss: 23097577053.4400\n",
      "Epoch [82/200], Train Loss: 150703.8245, Validation Loss: 20070204250.3670\n",
      "Epoch [83/200], Train Loss: 153115.8848, Validation Loss: 13613620766.3754\n",
      "Epoch [84/200], Train Loss: 148368.1048, Validation Loss: 2369654757.4911\n",
      "Epoch [85/200], Train Loss: 149547.2601, Validation Loss: 39384980729.4611\n",
      "Epoch [86/200], Train Loss: 150819.3968, Validation Loss: 46049715105.0332\n",
      "Epoch [87/200], Train Loss: 152449.1470, Validation Loss: 15440740052.1534\n",
      "Epoch [88/200], Train Loss: 150270.3077, Validation Loss: 49893805106.2712\n",
      "Epoch [89/200], Train Loss: 150191.2259, Validation Loss: 39998303015.6066\n",
      "Epoch [90/200], Train Loss: 149509.1801, Validation Loss: 41473679761.6208\n",
      "Epoch [91/200], Train Loss: 148588.2250, Validation Loss: 3449154531.4875\n",
      "Epoch [92/200], Train Loss: 152602.1047, Validation Loss: 36115321258.2787\n",
      "Epoch [93/200], Train Loss: 151347.5040, Validation Loss: 27952166069.0525\n",
      "Epoch [94/200], Train Loss: 150154.6093, Validation Loss: 42800905393.2551\n",
      "Epoch [95/200], Train Loss: 151106.2110, Validation Loss: 7453663456.2074\n",
      "Epoch [96/200], Train Loss: 152056.2038, Validation Loss: 114254187767.6951\n",
      "Epoch [97/200], Train Loss: 149021.9142, Validation Loss: 39657150506.2854\n",
      "Epoch [98/200], Train Loss: 152003.1427, Validation Loss: 30089344977.3452\n",
      "Epoch [99/200], Train Loss: 149888.0152, Validation Loss: 7862232595.3217\n",
      "Epoch [100/200], Train Loss: 149644.7343, Validation Loss: 33135780599.4593\n",
      "Epoch [101/200], Train Loss: 149645.1212, Validation Loss: 11661793189.2597\n",
      "Epoch [102/200], Train Loss: 148787.8713, Validation Loss: 35401495583.1750\n",
      "Epoch [103/200], Train Loss: 151560.6576, Validation Loss: 22783744211.8851\n",
      "Epoch [104/200], Train Loss: 149979.8024, Validation Loss: 49812074338.6894\n",
      "Epoch [105/200], Train Loss: 147987.7350, Validation Loss: 8760081349.5184\n",
      "Epoch [106/200], Train Loss: 150126.5121, Validation Loss: 18787788519.4033\n",
      "Epoch [107/200], Train Loss: 150391.9872, Validation Loss: 21251330389.0744\n",
      "Epoch [108/200], Train Loss: 147794.2799, Validation Loss: 37360616693.6399\n",
      "Epoch [109/200], Train Loss: 148246.4249, Validation Loss: 63983963796.5186\n",
      "Epoch [110/200], Train Loss: 150195.6112, Validation Loss: 31785590499.1421\n",
      "Epoch [111/200], Train Loss: 151451.6521, Validation Loss: 18668563920.6924\n",
      "Epoch [112/200], Train Loss: 150942.0367, Validation Loss: 31103332613.9060\n",
      "Epoch [113/200], Train Loss: 150432.8823, Validation Loss: 11297481915.3705\n",
      "Epoch [114/200], Train Loss: 153951.7391, Validation Loss: 33351546045.9144\n",
      "Epoch [115/200], Train Loss: 150237.0992, Validation Loss: 28808980330.2897\n",
      "Epoch [116/200], Train Loss: 147960.4545, Validation Loss: 25447903305.2758\n",
      "Epoch [117/200], Train Loss: 151212.5044, Validation Loss: 14996580247.8034\n",
      "Epoch [118/200], Train Loss: 154519.3221, Validation Loss: 31407080603.7670\n",
      "Epoch [119/200], Train Loss: 148278.6364, Validation Loss: 78481779319.9822\n",
      "Epoch [120/200], Train Loss: 150284.3965, Validation Loss: 32117385661.6643\n",
      "Epoch [121/200], Train Loss: 151100.0962, Validation Loss: 30709633309.6034\n",
      "Epoch [122/200], Train Loss: 149045.9241, Validation Loss: 18037280903.6477\n",
      "Epoch [123/200], Train Loss: 148413.5927, Validation Loss: 25055793047.1637\n",
      "Epoch [124/200], Train Loss: 150228.5729, Validation Loss: 11444476313.1657\n",
      "Epoch [125/200], Train Loss: 151363.3110, Validation Loss: 51341109954.4117\n",
      "Epoch [126/200], Train Loss: 149572.8041, Validation Loss: 4554784687.8711\n",
      "Epoch [127/200], Train Loss: 149772.2310, Validation Loss: 148058804981.6317\n",
      "Epoch [128/200], Train Loss: 152781.9724, Validation Loss: 23193836977.3951\n",
      "Epoch [129/200], Train Loss: 147504.7449, Validation Loss: 8894538739.1136\n",
      "Epoch [130/200], Train Loss: 150739.7461, Validation Loss: 15848924671.2168\n",
      "Epoch [131/200], Train Loss: 148574.1860, Validation Loss: 25874710353.1149\n",
      "Epoch [132/200], Train Loss: 151390.6010, Validation Loss: 9728313884.0957\n",
      "Epoch [133/200], Train Loss: 148936.1041, Validation Loss: 30041753388.3308\n",
      "Epoch [134/200], Train Loss: 146177.6346, Validation Loss: 51796155795.4162\n",
      "Epoch [135/200], Train Loss: 146745.6922, Validation Loss: 8578624933.9881\n",
      "Epoch [136/200], Train Loss: 153582.8926, Validation Loss: 14435089404.0655\n",
      "Epoch [137/200], Train Loss: 151650.9149, Validation Loss: 24117390144.2588\n",
      "Epoch [138/200], Train Loss: 150447.0792, Validation Loss: 31931684122.2583\n",
      "Epoch [139/200], Train Loss: 149302.6350, Validation Loss: 21832130056.4688\n",
      "Epoch [140/200], Train Loss: 153129.3442, Validation Loss: 29231636917.1371\n",
      "Epoch [141/200], Train Loss: 152213.2100, Validation Loss: 37916784879.8746\n",
      "Epoch [142/200], Train Loss: 153954.7974, Validation Loss: 9608110516.0524\n",
      "Epoch [143/200], Train Loss: 152218.1760, Validation Loss: 18395589814.3025\n",
      "Epoch [144/200], Train Loss: 149918.1407, Validation Loss: 28152036435.7388\n",
      "Epoch [145/200], Train Loss: 151843.9452, Validation Loss: 8616919454.4083\n",
      "Epoch [146/200], Train Loss: 151490.0624, Validation Loss: 28661988031.4122\n",
      "Epoch [147/200], Train Loss: 147034.0315, Validation Loss: 52638338920.8468\n",
      "Epoch [148/200], Train Loss: 153554.0401, Validation Loss: 27959404943.0414\n",
      "Epoch [149/200], Train Loss: 154149.5470, Validation Loss: 32954678765.2551\n",
      "Epoch [150/200], Train Loss: 153324.2517, Validation Loss: 32791996202.5230\n",
      "Epoch [151/200], Train Loss: 152317.8109, Validation Loss: 45499963455.5401\n",
      "Epoch [152/200], Train Loss: 152600.9922, Validation Loss: 37586790404.1054\n",
      "Epoch [153/200], Train Loss: 150839.9421, Validation Loss: 54440713563.8333\n",
      "Epoch [154/200], Train Loss: 150801.2444, Validation Loss: 13261011331.2753\n",
      "Epoch [155/200], Train Loss: 150538.4221, Validation Loss: 27339850769.2533\n",
      "Epoch [156/200], Train Loss: 148725.8352, Validation Loss: 13354108998.2832\n",
      "Epoch [157/200], Train Loss: 148364.6556, Validation Loss: 13942468274.3980\n",
      "Epoch [158/200], Train Loss: 152438.0041, Validation Loss: 60545630369.1857\n",
      "Epoch [159/200], Train Loss: 152706.3186, Validation Loss: 19900286698.7702\n",
      "Epoch [160/200], Train Loss: 152724.0300, Validation Loss: 20867051697.4640\n",
      "Epoch [161/200], Train Loss: 150654.8111, Validation Loss: 37453295375.1644\n",
      "Epoch [162/200], Train Loss: 151423.9193, Validation Loss: 31314294471.0928\n",
      "Epoch [163/200], Train Loss: 152374.3586, Validation Loss: 37610951129.0634\n",
      "Epoch [164/200], Train Loss: 152255.1878, Validation Loss: 61401874024.1861\n",
      "Epoch [165/200], Train Loss: 151957.2847, Validation Loss: 35636423543.7942\n",
      "Epoch [166/200], Train Loss: 151624.6519, Validation Loss: 12428545497.1792\n",
      "Epoch [167/200], Train Loss: 150862.6516, Validation Loss: 73873115996.6709\n",
      "Epoch [168/200], Train Loss: 151274.1919, Validation Loss: 5105685375.8202\n",
      "Epoch [169/200], Train Loss: 148501.5658, Validation Loss: 27687706961.1313\n",
      "Epoch [170/200], Train Loss: 151848.1546, Validation Loss: 30350249622.9671\n",
      "Epoch [171/200], Train Loss: 151224.0762, Validation Loss: 49020821959.8831\n",
      "Epoch [172/200], Train Loss: 154428.7802, Validation Loss: 33783507172.1227\n",
      "Epoch [173/200], Train Loss: 152768.6915, Validation Loss: 7792531794.0505\n",
      "Epoch [174/200], Train Loss: 151286.1466, Validation Loss: 8188471926.7774\n",
      "Epoch [175/200], Train Loss: 152479.6087, Validation Loss: 24080723309.8056\n",
      "Epoch [176/200], Train Loss: 153287.5614, Validation Loss: 46586823580.2411\n",
      "Epoch [177/200], Train Loss: 149251.0451, Validation Loss: 22640538228.3999\n",
      "Epoch [178/200], Train Loss: 150860.5327, Validation Loss: 18325061203.1981\n",
      "Epoch [179/200], Train Loss: 151757.0605, Validation Loss: 34777765666.0333\n",
      "Epoch [180/200], Train Loss: 149612.0824, Validation Loss: 8929984931.9809\n",
      "Epoch [181/200], Train Loss: 149073.0302, Validation Loss: 36646419403.2592\n",
      "Epoch [182/200], Train Loss: 150527.6981, Validation Loss: 32259387294.4824\n",
      "Epoch [183/200], Train Loss: 151040.0231, Validation Loss: 21099171820.5488\n",
      "Epoch [184/200], Train Loss: 151369.3961, Validation Loss: 29253874805.9902\n",
      "Epoch [185/200], Train Loss: 151891.2488, Validation Loss: 42720128533.7615\n",
      "Epoch [186/200], Train Loss: 148819.6732, Validation Loss: 27814045988.0259\n",
      "Epoch [187/200], Train Loss: 152980.7665, Validation Loss: 88990911561.3566\n",
      "Epoch [188/200], Train Loss: 150997.1977, Validation Loss: 31111129694.5037\n",
      "Epoch [189/200], Train Loss: 149433.7623, Validation Loss: 22485662765.8070\n",
      "Epoch [190/200], Train Loss: 149557.7284, Validation Loss: 36519026430.8255\n",
      "Epoch [191/200], Train Loss: 152132.8455, Validation Loss: 36087274002.0805\n",
      "Epoch [192/200], Train Loss: 146266.5930, Validation Loss: 24100783925.8073\n",
      "Epoch [193/200], Train Loss: 149099.9004, Validation Loss: 22635444758.1665\n",
      "Epoch [194/200], Train Loss: 147714.7139, Validation Loss: 23972302770.5139\n",
      "Epoch [195/200], Train Loss: 151747.3642, Validation Loss: 54057694867.0666\n",
      "Epoch [196/200], Train Loss: 150632.2725, Validation Loss: 38722127992.5750\n",
      "Epoch [197/200], Train Loss: 153775.4686, Validation Loss: 29038610674.8685\n",
      "Epoch [198/200], Train Loss: 151424.2353, Validation Loss: 28734185287.7349\n",
      "Epoch [199/200], Train Loss: 150396.3983, Validation Loss: 57693793905.1228\n",
      "Epoch [200/200], Train Loss: 149299.3473, Validation Loss: 2107902616.4575\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "import xgboost as xgb\n",
    "import torch.nn as nn\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size//4)\n",
    "        self.fc4 = nn.Linear(hidden_size//4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 254\n",
    "model = RegressionModel(input_size=7, hidden_size=hidden_size)\n",
    "\n",
    "#hyper\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "save_path = \"./weight\"\n",
    "\n",
    "log = []\n",
    "\n",
    "#train\n",
    "for epoch in range(num_epochs) :\n",
    "    model.train().cuda()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader :\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # validate\n",
    "    model.eval().cuda()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad() : \n",
    "        for inputs, targets in val_loader : \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # averg \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    log.append([epoch, avg_train_loss, avg_val_loss])\n",
    "    # scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 3\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-PCIE-32GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (0.03 MB) transferred to GPU in 0.000497 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 550.370117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(device='gpu')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb = LGBMRegressor(device='gpu')\n",
    "lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1146.9633386094592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_val, pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autogluon.core as ag\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(69) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(df)\n",
    "\n",
    "label = 'epoch_time'\n",
    "eval_metric = None\n",
    "time_limit = 3600*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240129_082908\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 21600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20240129_082908/ds_sub_fit/sub_fit_ho.\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240129_082908/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.5\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #99-Ubuntu SMP Mon Oct 30 20:42:41 UTC 2023\n",
      "CPU Count:          64\n",
      "Memory Avail:       402.68 GB / 440.29 GB (91.5%)\n",
      "Disk Space Avail:   706.69 GB / 878.65 GB (80.4%)\n",
      "===================================================\n",
      "Train Data Rows:    8888\n",
      "Train Data Columns: 7\n",
      "Label Column:       epoch_time\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    412342.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.42 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 4): ['flops', 'img_size', 'model_param_num', 'Tesla_V100']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['v_img_num']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['v_img_num']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 2 | ['t_img_num', 'batch_size']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 2 | ['t_img_num', 'batch_size']\n",
      "\t0.4s = Fit runtime\n",
      "\t2 features in original data used to generate 2 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.39s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1799.42s of the 5399.58s of remaining time.\n",
      "\t-231.6406\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1799.05s of the 5399.21s of remaining time.\n",
      "\t-219.1889\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1798.96s of the 5399.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-28.5754\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.92s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1783.79s of the 5383.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.7746\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.78s\t = Training   runtime\n",
      "\t2.15s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1767.3s of the 5367.46s of remaining time.\n",
      "\t-0.3997\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1764.82s of the 5364.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7668\t = Validation score   (-root_mean_squared_error)\n",
      "\t388.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1373.97s of the 4974.13s of remaining time.\n",
      "\t-0.1204\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1371.27s of the 4971.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-11.7787\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.95s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1302.64s of the 4902.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.8317\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.33s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1280.63s of the 4880.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-23.5806\t = Validation score   (-root_mean_squared_error)\n",
      "\t179.28s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1098.66s of the 4698.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.7715\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.55s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1059.62s of the 4659.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7595\t = Validation score   (-root_mean_squared_error)\n",
      "\t170.66s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 886.3s of the 4486.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-19.2178\t = Validation score   (-root_mean_squared_error)\n",
      "\t422.85s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 460.68s of the 4060.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-216.932\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 455.04s of the 4055.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-8.8189\t = Validation score   (-root_mean_squared_error)\n",
      "\t96.6s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 355.65s of the 3955.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.787\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.0s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 192.94s of the 3793.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-213.1999\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 187.68s of the 3787.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-10.4791\t = Validation score   (-root_mean_squared_error)\n",
      "\t185.05s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3599.97s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.925, 'RandomForestMSE_BAG_L1': 0.075}\n",
      "\t-0.1162\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1599.12s of the 3598.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-3.0066\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.22s\t = Training   runtime\n",
      "\t2.5s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1581.28s of the 3580.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.6438\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.09s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1569.01s of the 3568.71s of remaining time.\n",
      "\t-0.4877\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1565.68s of the 3565.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.8401\t = Validation score   (-root_mean_squared_error)\n",
      "\t616.91s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 946.01s of the 2945.7s of remaining time.\n",
      "\t-0.5315\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 943.22s of the 2942.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-8.1231\t = Validation score   (-root_mean_squared_error)\n",
      "\t66.48s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 874.03s of the 2873.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.9921\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.59s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 841.67s of the 2841.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-11.0012\t = Validation score   (-root_mean_squared_error)\n",
      "\t163.73s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 675.21s of the 2674.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.4539\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.48s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 665.73s of the 2665.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.9967\t = Validation score   (-root_mean_squared_error)\n",
      "\t541.48s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 121.44s of the 2121.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-24.9561\t = Validation score   (-root_mean_squared_error)\n",
      "\t131.69s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1986.04s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L2': 0.68, 'ExtraTreesMSE_BAG_L2': 0.27, 'XGBoost_BAG_L2': 0.02, 'LightGBMLarge_BAG_L2': 0.02, 'LightGBMXT_BAG_L2': 0.01}\n",
      "\t-0.4713\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L3 models ...\n",
      "Fitting model: LightGBMXT_BAG_L3 ... Training model for up to 1323.23s of the 1985.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-3.5148\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.33s\t = Training   runtime\n",
      "\t2.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L3 ... Training model for up to 1308.48s of the 1970.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.7722\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.53s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L3 ... Training model for up to 1292.6s of the 1954.64s of remaining time.\n",
      "\t-0.6734\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L3 ... Training model for up to 1289.77s of the 1951.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.442\t = Validation score   (-root_mean_squared_error)\n",
      "\t608.56s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L3 ... Training model for up to 678.46s of the 1340.5s of remaining time.\n",
      "\t-0.6918\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L3 ... Training model for up to 676.03s of the 1338.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-7.9654\t = Validation score   (-root_mean_squared_error)\n",
      "\t62.25s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 611.09s of the 1273.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.9518\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.94s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 584.33s of the 1246.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-10.5481\t = Validation score   (-root_mean_squared_error)\n",
      "\t168.56s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 413.07s of the 1075.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.6245\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.2s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L3 ... Training model for up to 404.93s of the 1066.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.4668\t = Validation score   (-root_mean_squared_error)\n",
      "\t332.66s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L3 ... Training model for up to 69.44s of the 731.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-38.9084\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.09s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.0s of the 637.16s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L3': 0.621, 'ExtraTreesMSE_BAG_L3': 0.273, 'LightGBMLarge_BAG_L3': 0.061, 'XGBoost_BAG_L3': 0.045}\n",
      "\t-0.6496\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L4 models ...\n",
      "Fitting model: LightGBMXT_BAG_L4 ... Training model for up to 636.39s of the 636.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-3.6448\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.0s\t = Training   runtime\n",
      "\t1.88s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L4 ... Training model for up to 621.05s of the 620.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.8007\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.11s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L4 ... Training model for up to 604.38s of the 604.3s of remaining time.\n",
      "\t-0.8042\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L4 ... Training model for up to 601.47s of the 601.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.3775\t = Validation score   (-root_mean_squared_error)\n",
      "\t489.9s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L4 ... Training model for up to 108.73s of the 108.66s of remaining time.\n",
      "\t-0.7878\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L4 ... Training model for up to 106.22s of the 106.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-6.2708\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.02s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L4 ... Training model for up to 38.49s of the 38.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-2.0299\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.66s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L4 ... Training model for up to 14.07s of the 14.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L4.\n",
      "Fitting model: WeightedEnsemble_L5 ... Training model for up to 360.0s of the -36.31s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.906, 'RandomForestMSE_BAG_L1': 0.073, 'ExtraTreesMSE_BAG_L2': 0.01, 'ExtraTreesMSE_BAG_L3': 0.01}\n",
      "\t-0.1156\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5437.8s ... Best model: \"WeightedEnsemble_L5\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240129_082908/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                          model  holdout_score   score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0          ExtraTreesMSE_BAG_L1      -0.097522   -0.120372  root_mean_squared_error        0.536262       0.841459     1.234356                 0.536262                0.841459           1.234356            1       True          7\n",
      "1           WeightedEnsemble_L2      -0.098259   -0.116248  root_mean_squared_error        1.123020       1.643302     3.348397                 0.002955                0.001018           1.016527            2       True         19\n",
      "2           WeightedEnsemble_L5      -0.168585   -0.115627  root_mean_squared_error       11.526716      14.022218  3332.081098                 0.004312                0.001034           1.446123            5       True         51\n",
      "3        RandomForestMSE_BAG_L1      -0.399935   -0.399696  root_mean_squared_error        0.583804       0.800826     1.097513                 0.583804                0.800826           1.097513            1       True          5\n",
      "4          CatBoost_r177_BAG_L1      -0.698106   -1.759462  root_mean_squared_error        0.066819       0.085872   170.664962                 0.066819                0.085872         170.664962            1       True         12\n",
      "5               CatBoost_BAG_L1      -0.699223   -1.766839  root_mean_squared_error        0.162172       0.092891   388.008721                 0.162172                0.092891         388.008721            1       True          6\n",
      "6            CatBoost_r9_BAG_L1      -0.700861   -1.786986  root_mean_squared_error        0.424461       0.268724   160.001733                 0.424461                0.268724         160.001733            1       True         16\n",
      "7          LightGBMLarge_BAG_L1      -0.737333   -1.771544  root_mean_squared_error        1.239931       1.219951    32.552648                 1.239931                1.219951          32.552648            1       True         11\n",
      "8               LightGBM_BAG_L1      -0.738866   -1.774596  root_mean_squared_error        1.540856       2.148532    12.782980                 1.540856                2.148532          12.782980            1       True          4\n",
      "9        RandomForestMSE_BAG_L2      -1.025142   -0.487689  root_mean_squared_error        7.720767       8.149540  1748.758284                 0.391522                0.889244           1.813255            2       True         22\n",
      "10          WeightedEnsemble_L3      -2.136485   -0.471314  root_mean_squared_error        9.827086      11.797119  1800.830897                 0.003479                0.000925           0.665725            3       True         31\n",
      "11       RandomForestMSE_BAG_L3      -2.624378   -0.673363  root_mean_squared_error       11.514701      14.199852  3330.846248                 0.272071                0.979759           1.300294            3       True         34\n",
      "12               XGBoost_BAG_L3      -2.874314   -1.951809  root_mean_squared_error       11.729175      13.339903  3353.481840                 0.486545                0.119810          23.935886            3       True         38\n",
      "13       RandomForestMSE_BAG_L4      -3.194274   -0.804177  root_mean_squared_error       15.534863      20.269764  4649.407406                 0.426451                0.900503           1.354703            4       True         46\n",
      "14              LightGBM_BAG_L4      -3.202860   -1.800710  root_mean_squared_error       15.774991      20.255072  4661.158871                 0.666579                0.885811          13.106169            4       True         45\n",
      "15          WeightedEnsemble_L4      -3.223946   -0.649583  root_mean_squared_error       12.426203      15.218547  3361.800771                 0.008178                0.000879           0.732521            4       True         43\n",
      "16              CatBoost_BAG_L4      -3.331408   -2.377487  root_mean_squared_error       15.239302      19.465274  5137.954973                 0.130890                0.096013         489.902271            4       True         47\n",
      "17               XGBoost_BAG_L4      -3.494095   -2.029868  root_mean_squared_error       15.558851      19.479650  4669.713183                 0.450439                0.110389          21.660480            4       True         50\n",
      "18              LightGBM_BAG_L3      -3.536915   -1.772241  root_mean_squared_error       11.918952      14.056812  3342.074188                 0.676322                0.836720          12.528234            3       True         33\n",
      "19         ExtraTreesMSE_BAG_L4      -3.596206   -0.787817  root_mean_squared_error       15.382151      20.225559  4649.136884                 0.273739                0.856298           1.084181            4       True         48\n",
      "20            LightGBMXT_BAG_L4      -3.784859   -3.644844  root_mean_squared_error       16.310336      21.246232  4660.055221                 1.201924                1.876971          12.002518            4       True         44\n",
      "21              CatBoost_BAG_L3      -3.878483   -2.441973  root_mean_squared_error       11.376574      13.330519  3938.104693                 0.133945                0.110427         608.558739            3       True         35\n",
      "22         CatBoost_r177_BAG_L3      -4.006966   -2.466822  root_mean_squared_error       11.323065      13.321122  3662.205056                 0.080436                0.101030         332.659102            3       True         41\n",
      "23         LightGBMLarge_BAG_L3      -4.114980   -1.624452  root_mean_squared_error       11.379634      13.317007  3334.743048                 0.137005                0.096914           5.197094            3       True         40\n",
      "24               XGBoost_BAG_L2      -4.417707   -1.992110  root_mean_squared_error        7.804000       7.398327  1776.531888                 0.474755                0.138031          29.586859            2       True         26\n",
      "25         ExtraTreesMSE_BAG_L2      -4.442426   -0.531541  root_mean_squared_error        7.616927       8.182505  1748.066446                 0.287682                0.922209           1.121417            2       True         24\n",
      "26            LightGBMXT_BAG_L3      -4.467206   -3.514789  root_mean_squared_error       12.295848      15.604068  3340.878853                 1.053219                2.383975          11.332899            3       True         32\n",
      "27         ExtraTreesMSE_BAG_L3      -4.500650   -0.691807  root_mean_squared_error       11.522404      14.021184  3330.634975                 0.279775                0.801091           1.089021            3       True         36\n",
      "28       NeuralNetFastAI_BAG_L4      -4.577037   -6.270788  root_mean_squared_error       15.391278      19.561346  4713.067870                 0.282866                0.192086          65.015168            4       True         49\n",
      "29            LightGBMXT_BAG_L2      -5.058476   -3.006605  root_mean_squared_error        8.557491       9.760751  1761.162876                 1.228246                2.500455          14.217847            2       True         20\n",
      "30         CatBoost_r177_BAG_L2      -5.208387   -2.996692  root_mean_squared_error        7.444569       7.364460  2288.428339                 0.115324                0.104164         541.483310            2       True         29\n",
      "31    NeuralNetTorch_r22_BAG_L1      -5.343812  -10.479082  root_mean_squared_error        0.088978       0.129612   185.051341                 0.088978                0.129612         185.051341            1       True         18\n",
      "32       NeuralNetFastAI_BAG_L3      -5.382428   -7.965355  root_mean_squared_error       11.514122      13.412370  3391.797162                 0.271493                0.192278          62.251208            3       True         37\n",
      "33              CatBoost_BAG_L2      -5.477002   -2.840110  root_mean_squared_error        7.472598       7.382134  2363.853604                 0.143353                0.121838         616.908575            2       True         23\n",
      "34  NeuralNetFastAI_r191_BAG_L1      -5.620284   -8.818860  root_mean_squared_error        0.275202       0.210563    96.600193                 0.275202                0.210563          96.600193            1       True         15\n",
      "35       NeuralNetFastAI_BAG_L2      -6.380891   -8.123055  root_mean_squared_error        7.665039       7.456549  1813.427215                 0.335794                0.196253          66.482186            2       True         25\n",
      "36              LightGBM_BAG_L2      -6.551342   -1.643847  root_mean_squared_error        7.611634       7.590737  1756.033523                 0.282389                0.330441           9.088494            2       True         21\n",
      "37         LightGBMLarge_BAG_L2      -7.292558   -1.453898  root_mean_squared_error        7.441402       7.346255  1753.425795                 0.112157                0.085959           6.480766            2       True         28\n",
      "38        NeuralNetTorch_BAG_L2      -8.277132  -11.001216  root_mean_squared_error        7.583124       7.588698  1910.672272                 0.253879                0.328402         163.727243            2       True         27\n",
      "39       NeuralNetFastAI_BAG_L1      -8.814080  -11.778706  root_mean_squared_error        1.067301       0.211532    65.949929                 1.067301                0.211532          65.949929            1       True          8\n",
      "40        NeuralNetTorch_BAG_L3      -9.018900  -10.548059  root_mean_squared_error       11.440428      13.467524  3498.107687                 0.197799                0.247431         168.561733            3       True         39\n",
      "41    NeuralNetTorch_r79_BAG_L1      -9.164084  -19.217751  root_mean_squared_error        0.100391       0.109888   422.847741                 0.100391                0.109888         422.847741            1       True         13\n",
      "42    NeuralNetTorch_r79_BAG_L2     -13.163226  -24.956080  root_mean_squared_error        7.617528       7.603097  1878.636003                 0.288283                0.342801         131.690974            2       True         30\n",
      "43    NeuralNetTorch_r79_BAG_L3     -17.519880  -38.908449  root_mean_squared_error       11.519802      13.499825  3420.638491                 0.277173                0.279733          91.092537            3       True         42\n",
      "44        NeuralNetTorch_BAG_L1     -19.167144  -23.580560  root_mean_squared_error        0.074841       0.099001   179.275249                 0.074841                0.099001         179.275249            1       True         10\n",
      "45            LightGBMXT_BAG_L1     -28.226229  -28.575419  root_mean_squared_error        0.466837       0.590252     5.915254                 0.466837                0.590252           5.915254            1       True          3\n",
      "46               XGBoost_BAG_L1     -72.922287   -1.831668  root_mean_squared_error        0.483897       0.124477    19.329705                 0.483897                0.124477          19.329705            1       True          9\n",
      "47        KNeighborsDist_BAG_L1    -196.874299 -219.188885  root_mean_squared_error        0.008757       0.060294     0.016746                 0.008757                0.060294           0.016746            1       True          2\n",
      "48        KNeighborsUnif_BAG_L1    -199.327040 -231.640570  root_mean_squared_error        0.013444       0.056089     0.284140                 0.013444                0.056089           0.284140            1       True          1\n",
      "49          LightGBM_r96_BAG_L1    -224.262907 -213.199861  root_mean_squared_error        0.122776       0.146047     2.466378                 0.122776                0.146047           2.466378            1       True         17\n",
      "50         LightGBM_r131_BAG_L1    -228.987721 -216.931960  root_mean_squared_error        0.072516       0.064288     2.865441                 0.072516                0.064288           2.865441            1       True         14\n",
      "Stacked overfitting occurred: True.\n",
      "Spend 5458 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 16142 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 16142s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240129_082908\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.5\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #99-Ubuntu SMP Mon Oct 30 20:42:41 UTC 2023\n",
      "CPU Count:          64\n",
      "Memory Avail:       401.00 GB / 440.29 GB (91.1%)\n",
      "Disk Space Avail:   706.69 GB / 878.65 GB (80.4%)\n",
      "===================================================\n",
      "Train Data Rows:    10000\n",
      "Train Data Columns: 7\n",
      "Label Column:       epoch_time\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    410627.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.47 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 4): ['flops', 'img_size', 'model_param_num', 'Tesla_V100']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['v_img_num']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['v_img_num']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 2 | ['t_img_num', 'batch_size']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 2 | ['t_img_num', 'batch_size']\n",
      "\t0.3s = Fit runtime\n",
      "\t2 features in original data used to generate 2 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 16141.62s of the 16141.6s of remaining time.\n",
      "\t-252.6168\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 16141.52s of the 16141.5s of remaining time.\n",
      "\t-229.6556\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 16141.43s of the 16141.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-28.6138\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.8s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 16132.63s of the 16132.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.7358\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.57s\t = Training   runtime\n",
      "\t2.85s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 16115.61s of the 16115.58s of remaining time.\n",
      "\t-0.3648\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 16113.12s of the 16113.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7107\t = Validation score   (-root_mean_squared_error)\n",
      "\t259.82s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 15850.64s of the 15850.61s of remaining time.\n",
      "\t-0.1045\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 15847.76s of the 15847.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-10.4352\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.75s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 15773.2s of the 15773.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-24.1599\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.04s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 15751.42s of the 15751.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-20.6701\t = Validation score   (-root_mean_squared_error)\n",
      "\t216.84s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 15531.93s of the 15531.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-1.7342\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.56s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 15495.79s of the 15495.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7119\t = Validation score   (-root_mean_squared_error)\n",
      "\t178.48s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 15314.62s of the 15314.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-13.8441\t = Validation score   (-root_mean_squared_error)\n",
      "\t541.23s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 14770.75s of the 14770.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-217.7102\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 14765.38s of the 14765.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-7.9584\t = Validation score   (-root_mean_squared_error)\n",
      "\t106.36s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 14656.21s of the 14656.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7296\t = Validation score   (-root_mean_squared_error)\n",
      "\t124.91s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 14528.53s of the 14528.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-214.3319\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.56s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 14523.13s of the 14523.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-8.4171\t = Validation score   (-root_mean_squared_error)\n",
      "\t349.76s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 14170.65s of the 14170.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-217.5233\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.26s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 14153.67s of the 14153.65s of remaining time.\n",
      "\t-5.9375\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 14151.05s of the 14151.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.752\t = Validation score   (-root_mean_squared_error)\n",
      "\t552.87s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 13595.49s of the 13595.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-20.195\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.43s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 13561.31s of the 13561.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.6936\t = Validation score   (-root_mean_squared_error)\n",
      "\t439.04s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 13119.46s of the 13119.43s of remaining time.\n",
      "\t-2.5942\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 13116.58s of the 13116.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-28.4127\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.94s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 13108.65s of the 13108.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-12.4855\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.27s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 12970.57s of the 12970.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-215.62\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.03s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 12956.86s of the 12956.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-25.8195\t = Validation score   (-root_mean_squared_error)\n",
      "\t436.65s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 12517.48s of the 12517.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-216.5128\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 12512.67s of the 12512.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-13.2151\t = Validation score   (-root_mean_squared_error)\n",
      "\t263.71s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 12246.27s of the 12246.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7423\t = Validation score   (-root_mean_squared_error)\n",
      "\t597.74s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 11645.76s of the 11645.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-16.1497\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.22s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 11507.83s of the 11507.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-216.9817\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 11494.36s of the 11494.34s of remaining time.\n",
      "\t-3.9082\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 11492.08s of the 11492.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-1.7096\t = Validation score   (-root_mean_squared_error)\n",
      "\t428.11s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 11061.29s of the 11061.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-15.5247\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.42s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 10958.01s of the 10957.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-50.0247\t = Validation score   (-root_mean_squared_error)\n",
      "\t308.63s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 10646.66s of the 10646.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n",
      "\t-218.2773\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.61s\t = Training   runtime\n",
      "\t1.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 10608.05s of the 10608.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n",
      "\t-50.8479\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.73s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 10561.61s of the 10561.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=32, gpus=0, memory=0.00%)\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=label, problem_type='regression', eval_metric=eval_metric\n",
    ").fit(train_data, \n",
    "      presets='best_quality', \n",
    "      num_stack_levels=3,\n",
    "      time_limit=time_limit, num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictor.leaderboard(silent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.feature_importance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = predictor.get_model_best()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
